# 《计算机操作系统》

> 参考书籍：汤子瀛， 哲凤屏， 汤小丹. 计算机操作系统[M]. 西安电子科技大学出版社， 2001.
参考网站：https://github.com/CyC2018/CS-Notes/blob/master/notes/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%20-%20%E7%9B%AE%E5%BD%95.md

# 1 操作系统引论

## 1.1 操作系统的作用

**1.作为用户与计算机硬件系统之间的接口**

用户在OS帮助下能够方便、快捷、可靠操纵计算机硬件和运行自己的程序。图1-1是OS作为接口的示意图。由图可看出，用户可通过三种方式使用计算机，即通过命令方式、系统调用方式和图标—窗口方式来实现与操作系统的通信，并取得它的服务。

<img src="Image\image-20210710132410498.png" alt="image-20210710132410498" style="zoom: 67%;" />

**2.OS作为计算机系统资源的管理者**

资源分为四类:处理机、存储器、IO设备以及文件(数据和程序)。OS 的主要功能也正是对这四类资源进行有效的管理

**3.OS实现了对计算机资源的抽象**

为了方便用户使用IO 设备，人们在裸机上覆盖上一层IO设备管理软件，如图1-2所示，由它来实现对I/O设备操作的细节，并向上将I/O设备抽象为一组数据结构以及一组I/O操作命令，如read和 write命令，这样用户即可利用这些数据结构及操作命令来进行数据输入或输出，而无需关心IO是如何具体实现的。

<img src="Image\image-20210710132550165.png" alt="image-20210710132550165" style="zoom: 50%;" />

## 1.2 操作系统的发展

### 1.2.2 单道批处理系统

为实现对作业的连续处理，需要先把一批作业以脱机方式输入到磁带上，并在系统中配上监督程序(Monitor)，在它的控制下，使这批作业能一个接一个地连续处理

<img src="Image\image-20210710132837255.png" alt="image-20210710132837255" style="zoom:50%;" />

单道批处理系统的**缺点**：系统中的资源得不到充分的利用。这是因为在内存中仅有一道程序，每逢该程序在运行中发出IO请求后，CPU 便处于等待状态，必须在其IO 完成后才继续运行。又因IO 设备的低速性，更使CPU的利用率显著降低。图1-5示出了单道程序的运行情况，从图可以看出:在t2～t3、t6~t7 时间间隔内CPU空闲

<img src="Image\image-20210710132955455.png" alt="image-20210710132955455" style="zoom: 50%;" />

### 1.2.3 多道批处理系统

(Multiprogrammed Batch Processing System)

用户所提交的作业先存放在外存上，并排成一个队列，称为“**后备队列**”。然后由作业调度程序按一定的算法,从后备队列中选择若干个作业调入内存，使它们共享CPU和系统中的各种资源。由于同时在内存中装有若干道程序,这样便可以在运行程序A时,利用其因I/O操作而暂停执行时的CPU空档时间，再调度另一道程序B运行，使多道程序交替地运行

<img src="Image\image-20210710133131035.png" alt="image-20210710133131035" style="zoom:67%;" />

优缺点：

- 资源利用率高
- 系统吞吐量大
- 平均周转时间长
- 无交互能力



### 1.2.4 分时系统

(Time Sharing System)

分时系统是指，在一台主机上连接了多个配有显示器和键盘的终端并由此所组成的系统，该系统允许多个用户同时通过自己的终端，以交互方式使用计算机，共享主机中的资源

### 1.2.5 实时系统

(Real Time System)

实时系统是指系统能及时响应外部事件的请求，在规定的时间内完成对该事件的处理，并控制所有实时任务协调一致地运行。常见的有：工业控制系统、信息查询系统、多媒体系统、嵌入式系统

### 1.2.6 微机操作系统

- 单用户单任务操作系统，如 MS-DOS
- 单用户多任务操作系统，如 Windows
- 多用户多任务操作系统：允许多个用户通过各自的终端，使用同一台机器，共享主机系统中的各种资源，如 UNIX，基于此的 Linux



## 1.3 操作系统的基本特性

### 1.3.1 并发

(Concurrence)

并行性是指两个或多个事件在**同一时刻**发生

并发性是指两个或多个事件在**同一时间间隔**内发生

> 在多道程序环境下，并发性是指在一段时间内宏观上有多个程序在同时运行
>
> 在单处理机系统中，每一时刻却仅能有一道程序执行。例如，在1秒钟时间内，0～15 ms程序A运行;15~30 ms程序B运行;30~45 ms程序C运行;45~60 ms程序D运行，因此可以说，在1秒钟时间间隔内，宏观上有四道程序在同时运行，但微观上，程序A、B、C、D是分时地交替执行的，称为A B C D程序**并发运行**

### 1.3.2 共享

在OS 环境下的资源共享或称为**资源复用**，是指系统中的资源可供内存中多个并发执行的进程共同使用。这里在宏观上既限定了时间(进程在内存期间)，也限定了地点(内存)。目前主要实现资源共享的方式有如下两种：

1. 互斥共享方式

   规定在一段时间内，只允许一个进程访问该资源，如打印机、磁带机等。把这种在一段时间内只允许-一个进程访问的资源，称为**临界资源**。

2. 同时访问方式

   如磁盘设备。

### 1.3.3 虚拟

把通过某种技术将一个物理实体变为若干个逻辑上的对应物的功能称为“虚拟”。前者是实的，即实际存在的，而后者是虚的，是用户感觉上的东西。相应地，把用于实现虚拟的技术称为虚拟技术。在OS 中也是利用时分复用和空分复用技术来实现“虚拟”的。

### 1.3.4 异步

在多道程序环境下,允许多个进程并发执行，进程在使用资源时可能需要等待或放弃，进程的执行并不是一气成的,而是以走走停停的形式推进。进程以不可预知的速度向前推进。何时执行、何时暂停、何时完成都是未知的，这就造成了系统的异步性。

<img src="Image\20210313162838318.png" alt="20210313162838318" style="zoom: 67%;" />

## 1.4 操作系统的功能

**1. 处理机管理**：进程控制、进程同步、进程通信、处理机调度、死锁处理

**2. 存储器管理**：内存分配、内存保护与共享、地址映射、虚拟内存

**3. 设备管理**：缓冲管理、设备分配、设备处理

**4. 文件管理**：文件存储空间的管理、目录管理、文件读写管理和保护



## 1.5 OS 结构设计

**宏内核**：宏内核是将操作系统功能作为一个紧密结合的整体放到内核。由于各模块共享信息，因此有很高的性能。

**微内核**：由于操作系统不断复杂，因此将一部分操作系统功能移出内核，从而降低内核的复杂性。移出的部分根据分层的原则划分成若干服务，相互独立

### 1.5.4 微内核 OS 结构

在微内核操作系统中，内核是指精心设计的、**能实现现代OS最基本核心功能的小内核**，微内核并非是一个完整的OS，而只是将操作系统中最基本的部分放入微内核

基本功能：

1. 进程、线程管理
2. 低级存储器管理：如用于实现将用户空间的逻辑地址变换为内存空间的物理地址的页表机制和地址变换机制
3. 中断和陷入处理：捕获所发生的中断和陷入事件，并进行相应的前期处理

优点：提高可扩展性、增强可靠性、可移植性强

缺点：同比早期操作系统，运行效率降低。原因是，在完成一次客户对操作系统提出的服务请求时，需要利用消息实现多次交互和进行用户/内核模式与上下文的多次切换。例如：

1. 早期的操作系统只需要**两次上下文**的切换

如果一个进程在用户态需要使用内核态的功能，就进行系统调用从而陷入内核，由操作系统代为完成。

<img src="Image\1"  style="zoom:67%;" />

2. 微内核中需要**至少四次**：客户机发送请求给内核 => 内核把客户消息发送给文件系统 => 文件系统完成请求发送给内核 => 内核返回消息给客户机。实际上还会引起更多的上下文切换

   <img src="Image\2" style="zoom: 80%;" />

为了改善运行效率,可以重新把一些常用的操作系统基本功能由服务器移入微内核中。这样可使客户对常用操作系统功能的请求所发生的用户/内核模式和上下文的切换的次数由四次或八次降为两次

## 1.6 中断分类

**1. 外中断**：由 CPU 执行指令以外的事件引起，如 I/O 完成中断，表示设备输入/输出处理已经完成，处理器能够发送下一个输入/输出请求。此外还有时钟中断、控制台中断等。

**2. 异常**：由 CPU 执行指令的内部事件引起，如非法操作码、地址越界、算术溢出等。

**3. 陷入**：在用户程序中使用系统调用

# 进程的描述与控制

## 进程

**1. 进程**

进程是资源分配的基本单位。为了使参与并发执行的每个程序(含数据)都能独立地运行，在操作系统中必须为之配置一个专门的数据结构，称为**进程控制块(Process Control Block， PCB)**，用来描述进程的基本信息和运行状态，所谓的创建进程和撤销进程，都是指对 PCB 的操作。

下图显示了 4 个程序创建了 4 个进程，这 4 个进程可以并发地执行。

<img src="Image\3"  style="zoom:67%;" />

进程的特征：

- **动态性**。 它由创建而产生，由调度而执行，由撤消而消亡。可见，**进程实体有一定的生命期**，而程序则只是一组有序指令的集合，并存放于某种介质上，其本身并不具有活动的含义，因而是静态的。（**程序是存放于介质上的静态指令**）
- **并发性**。指**多个进程实体同存于内存中，且能在一段时间内同时运行**。引入进程的目的也正是为了使其进程实体能和其它进程实体并发执行
- **独立性**。独立性是指进程实体是一个能**独立运行、独立获得资源和独立接受调度**的基本单位
- **异步性**。是指进程是按异步方式运行的，即按各自独立的、不可预知的速度向前推进



## 进程状态的切换

进程主要的三种状态：

- **就绪(Ready)状态**。进程已分配到除CPU以外的所有必要资源后，只要再获得CPU，便可立即执行。所有就绪状态的进程，排成一个队列，称为**就绪队列**

- **执行(Running)状态**。进程已获得CPU，程序正在执行的状态。多处理机系统中，则有多个进程处于执行状态

- **阻塞(Block)状态**。指正在执行的进程**由于发生某事件**(如 IO 请求、申请缓冲区失败等)暂时无法继续执行时的状态

  > 此时引起进程调度， **os把处理机分配给另一个就绪进程，而让受阻进程处于暂停状态**，一般将这种暂停状态称为阻塞状态。
  >
  > 通常系统将处于阻塞状态的进程也排成队列，称该队列为**阻塞队列**。在较大的系统中，为了减少队列操作的开销，提高，系统效率等原因会设置多个阻塞队列。

<img src="Image\image-20210620121027188.png" alt="image-20210620121027188" style="zoom: 33%;" />

另外还有创建状态和终止状态：

- **创建状态**：首先由进程申请一个空白PCB，并向PCB中填写用于控制和管理进程的信息然后为该进程分配运行时所必须的资源;最后，把该进程转入就绪状态并插入就绪队列之中
- **终止状态**：等待操作系统进行善后处理，最后将其PCB清零，并将PCB空间返还系统

注意：

- 只有**就绪态和运行态可以相互转换**，其它的都是单向转换。就绪状态的进程通过调度算法从而**获得 CPU 时间**，转为运行状态；而运行状态的进程，在分配给它的 CPU 时间片用完之后就会转为就绪状态，等待下一次调度
- 阻塞状态是缺少需要的资源从而由运行状态转换而来，但是该资源不包括 CPU 时间，缺少 CPU 时间会从运行态转换为就绪态

## 挂起和激活操作

挂起操作：当该操作作用于某个进程时，该进程将被挂起，意味着此时该进程处于静止状态。如果进程正在执行，它将暂停执行。若原本处于就绪状态，则该进程此时暂不接受调度。与挂起操作对应的操作是激活操作。

挂起的原因：例如对程序运行存在疑问，希望暂停执行、父进程需要修改子进程等

当引入挂起或激活后，进程状态的转换如下：

1. 活动就绪 => 静止就绪，激活反之
2. 活动阻塞 => 静止阻塞，激活反之

<img src="Image\image-20210710142747870.png" alt="image-20210710142747870" style="zoom:50%;" />

## 进程控制块（PCB）

操作系统对于每个资源和每个进程都设置了一个数据结构，这些数据结构一般分为以下四类：内存表、设备表、文件表、进程表，如下所示。其中进程表又称为**进程控制块**。

<img src="Image\image-20210710143151466.png" alt="image-20210710143151466" style="zoom:50%;" />

### PCB 主要功能

- 作为独立运行基本单位的标志
- 实现间断性运行
- 提供进程调度所需要的信息
- 实现与其他进程同步与通信

### PCB 中的信息

1. **进程标识符**

   用于唯一的标识一个进程，包括外部标识符和内部标识符

2. **处理机状态**

   也称为处理机的上下文，有处理机的各种寄存器中的内容组成。例如指令计数器、程序状态字 PSW、用户栈指针。

   > 当进程被切换时，处理机状态信息必须保存在相应的PCB 中，以便在该进程重新执行时能再从断点继续执行

3. **进程调度信息**

   包含进程状态、进程优先级、调度算法中的相关信息、引起阻塞等待的事件

4. **进程控制信息**

   用于进程控制所必须的信息。包括程序和数据的地址、进程同步和通信机制、资源清单、连接指针。

### PCB 的组织方式

系统中通常可拥有数千个PCB。为了能对它们加以有效的管理，应该用适当的方式将这些PCB组织起来，有以下三种

- **线性方式**：将系统中所有的PCB都组织在一张线性表中，将该表的首址存放在内存的一个专用区域中
- **链接方式**：把具有相同状态进程的PCB分别通过 PCB 中的链接子链接成一个队列，形成就绪队列、若干个阻塞队列和空白队列等
- **索引方式**：系统根据所有进程状态的不同，建立几张索引表，例如，就绪索引表、阻塞索引表等，并把各索引表在内存的首地址记录在内存的一些专用单元中

<img src="Image\image-20210710144328150.png" alt="image-20210710144328150" style="zoom:50%;" />

<img src="Image\image-20210710144340549.png" alt="image-20210710144340549" style="zoom: 67%;" />

## 进程的控制

### 操作系统内核

为了防止OS本身及关键数据(如 PCB等)遭受到应用程序有意或无意的破坏，通常也将处理机的执行状态分成系统态和用户态两种

- **系统态**：又称为管态，也称为**内核态**。它具有较高的特权，能执行一切指令，访问所有寄存器和存储区，传统的OS都在系统态运行

- **用户态**：又称为目态。它是具有较低特权的执行状态，仅能执行规定的指令，访问指定的寄存器和存储区。一般情况下，应用程序只能在用户态运行，不能去执行OS指令及访问OS区域，这样可以防止应用程序对OS的破坏

内核态的功能：

1. 支撑功能

- **中断处理**。中断处理是内核最基本的功能
- **时钟管理**。如在时间片轮转调度中，每当时间片用完时，便由时钟管理产生一个中断信号，促使调度程序重新进行调度
- **原语操作**。所谓原语(Primitive)，就是**由若干条指令组成的，用于完成一定功能的一个过程**。它与一般过程的区别在于:它们是“原子操作(Action Operation)”。原语在执行过程中不允许被中断。原子操作在系统态下执行，常驻内存。在内核中可能有许多原语，如用于对链表进行操作的原语、用于实现进程同步的原语等。

2. 资源管理：包括进程管理、存储器管理、设备管理

### 进程的创建

在OS 中，允许一个进程创建另一个进程，通常把创建进程的进程称为**父进程**，而把被创建的进程称为**子进程**。子进程可继续创建更多的孙进程。

> 注：Windows中不存在任何进程层次结构的概念，所有的进程都具有相同的地位。如果一个进程创建另外的进程时创建进程获得了一个句柄，其作用相当于一个令牌，用来控制被创建的进程

进程的创建过程：

1. **申请空白PCB**，为新进程申请获得唯一的数字标识符，并从PCB集合中索取一个空白PCB。

2. **为新进程分配其运行所需的资源**，包括各种物理和逻辑资源，如内存、文件、IO设备和CPU时间等。这些资源或从操作系统或仅从其父进程获得。新进程对这些资源的需求详情一般也要提前告知操作系统或其父进程。例如，为新进程的程序和数据以及用户栈分配必要的内存空间时，操作系统必须知道新进程所需内存的大小

3. **初始化进程控制块(PCB)**。包括:

   ①初始化标识信息，将系统分配的标识符和父进程标识符填入新PCB 中;

   ②初始化处理机状态信息，使程序计数器指向程序的入口地址，使栈指针指向栈顶;

   ③初始化处理机控制信息，将进程的状态设置为就绪状态或静止就绪状态，并默认设置为最低优先级

4. **插入就绪队列**：如果进程就绪队列能够接纳新进程，便将新进程插入就绪队列

### 进程的终止

引起进程终止的事件

- 正常结束
- 异常结束，如非法指令、等待超时、运行超时、IO 故障等
- 外界干预，如系统死锁导致操作系统干预、父进程要求停止、父进程终止等

进程终止的过程：

1. 根据被终止进程的标识符，从PCB集合中检索出该进程的PCB,从中读出该进程的状态;
2. 若被终止进程正处于执行状态，应立即终止该进程的执行，并置调度标志为真，用于指示该进程被终止后应重新进行调度
3. 若该进程还有子孙进程，还应将其所有子孙进程也都予以终止，以防它们成为不可控的进程;
4. 将被终止进程所拥有的全部资源或者归还给其父进程，或者归还给系统
5. 将被终止进程(PCB)从所在队列(或链表)中移出，等待其它程序来搜集信息



### 进程的阻塞与唤醒

引起进程阻塞的事件

- 向系统请求共享资源失败。如已无在可分配的打印机
- 等待某种操作的完成
- 新数据尚未到达
- 等待新任务的到达

进程阻塞的过程：

1. 进程便通过调用阻塞原语 block 将自己阻塞。可见，阻塞是进程自身的一种**主动行为**。

2. 进入 block过程后，由于该进程还处于执行状态，所以应先立即停止执行，把进程控制块中的现行状态由“执行”改为阻塞，并将PCB插入根据事件分类的阻塞队列
3. 调度程序进行重新调度，将处理机分配给另一就绪进程，并进行切换

进程唤醒过程：

当被阻塞进程所期待的事件发生时，比如它所启动的IO操作已完成，或其所期待的数据已经到达，则由有关进程从该事件的阻塞队列中移出的进程唤醒。wakeup 执行的过程是:首先把被阻塞的进程从等待该事件的阻塞队列中移出，将其PCB中的现行状态由阻塞改为就绪，然后再将该PCB插入到就绪队列中。

> block 原语和 wakeup 原语是一对作用相反的原语，必须成对使用

### 进程的挂起与激活

进程的挂起：当系统中出现了引起进程挂起的事件时，OS将利用挂起原语suspend将指定进程或处于阻塞状态的进程挂起

进程的激活过程：当系统中发生激活进程的事件时，OS将利用激活原语active，将指定进程激活

## 进程同步【重要】

进程同步机制的主要任务，是对多个相关进程在执行次序上进行协调，使并发执行的诸进程之间能按照一定的规则(或时序)共享系统资源，并能很好地相互合作，从而使程序的执行具有可再现性。

同步的机制有：**硬件同步机制、信号量同步机制、管城同步机制**

### 基本概念

**1. 制约关系**：间接相互制约、直接相互制约

**2. 临界资源**：许多硬件资源如打印机、磁带机等，都属于临界资源，诸进程间应采取**互斥方式**，实现对这种资源的共享。

**3. 临界区**

对临界资源进行访问的那段代码称为**临界区(critical section)**，为了互斥访问临界资源，每个进程在进入临界区之前，需要先进行检查。

在临界区前面增加一段用于进行上述检查的代码，把这段代码称为**进入区(entry section)**。

相应地，在临界区后面也要加上一段称为**退出区(exit section)**的代码，用于将临界区正被访问的标志恢复为未被访问的标志。

进程中除上述进入区、临界区及退出区之外的其它部分的代码在这里都称为**剩余区**。

**4. 同步机制应遵循的原则**

空闲让进、忙则等待、有限等待、让权等待

### 硬件同步机制

在对临界区进行管理时，可以将标志看做一个锁，“锁开”进入，“锁关”等待，初始时锁是打开的。

每个要进入临界区的进程必须先对锁进行测试，当锁未开时，则必须等待，直至锁被打开。反之，当锁是打开的时候，则应立即把其锁上，以阻止其它进程进入临界区。

> 为防止多个进程同时测试到锁为打开的情况，测试和关锁操作必须是连续的，不允许分开进行。

1. **关中断**

关中断是实现互斥的最简单的方法之一。在进入锁测试之前关闭中断，直到完成锁测试并上锁之后才能打开中断。这样，进程在临界区执行期间，计算机系统不响应中断，从而不会引发调度，也就不会发生进程或线程切换。但缺点太大，不使用。

2. **利用Test-and-Set指令实现互斥**

借助一条硬件指令——“测试并建立”指令TS(Test-and-Set)以实现互斥的方法，指令如下，为一条原语，其中 lock 有两种状态：lock=False 表示资源空闲，lock=True 表示资源占用

```C
boolean TS(boolean * lock){
    Boolean old;
    old=*lock;
    *lock=True;
    return old;
}
```

进程在进入临界区之前，首先用TS 指令测试lock，如果其值为FALSE，则表示没有进程在临界区内，可以进入，并将TRUE值赋予lock，这等效于关闭了临界资源，使任何进程都不能进入临界区，否则必须循环测试直到TS(s)为TRUE。

3. **利用 Swap 指令实现进程互斥**

该指令称为对换指令，用于交换两个字的内容。其处理过程描述如下

```c
void swap(boolean *a,boolean *b){
    boolean temp;
    temp=*a;
    *a=*b;
    *b=temp;
}
```

用对换指令可以简单有效地实现互斥，方法是为每个临界资源设置一个全局的布尔变量lock，其初值为 false，在每个进程中再利用一个局部布尔变量key。利用Swap指令实现进程互斥的循环进程可描述如下:

```C
do{
    key=True;
    do{
        swap(&lock,&key);
    }while(key!=False);
        //临界区
    lock=False;
}while(True);
```

### 信号量机制

整型信号量（Semaphore）定义为一个用于表示资源数目的整型量S，它与一般整型量不同，除初始化外，仅能通过两个标准的**原子操作wait(S)** 和 **signa(S)** 来访问。很长时间以来，这两个操作一直被分别称为 **P， V** 操作。wait和signal操作可描述如下:

```C
wait(S){
    while(S<=0);//什么也不做
    S--;//原子性操作
}

signal(S){
    S++;//原子性操作
}
```

**利用信号量实现进程互斥**

为使多个进程能互斥地访问某临界资源，只需为该资源设置一**互斥信号量 mutex**，并设其初始值为1，然后将各进程访问该资源的临界区CS 置于 `wait(mutex) `和 `signa(mutex)` 操作之间即可。

> 每个欲访问该临界资源的进程在进入临界区之前，都要先对mutex执行wait操作，若该资源此刻未被访问，本次wait操作必然成功，进程便可进入自己的临界区，这时若再有其它进程也欲进入自己的临界区，由于对mutex执行wait操作定会失败，因而此时该进程阻塞，从而保证了该临界资源能被互斥地访问。当访问临界资源的进程退出临界区后，又应对mutex执行signal操作，以便释放该临界资源。

例如：定义 mutex 为互斥信号量初始值为 1，取值为 -1， 0， 1

- mutex=1 表示两个进程皆未进入需要互斥的临界区
- mutex=0 表示有一个进程进入临界区运行，另外一个必须等待，挂入阻塞队列
- mutex=-1 表示有一个进程正在临界区运行，另外一个进程因等待而阻塞在信号量队列中，需要被当前已在临界区运行的进程退出时唤醒。

```C
semaphore mutex=1;

//第一个程序
P1(){
    while(1){
        wait(mutex);
        临界区;
        signal(mutex);
        剩余区;
    }
}
//第二个程序
P2(){
    while(1){
        wait(mutex);
        临界区;
        signal(mutex);
        剩余区;
    }
}
```

### 管程

> 虽然信号量机制是一种既方便、又有效的进程同步机制，但每个要访问临界资源的进程都必须自备同步操作wait(S)和signal(s)。这就使大量的同步操作分散在各个进程中。这不仅给系统的管理带来了麻烦，而且还会因同步操作的使用不当而导致系统死锁。这样，在解决上述问题的过程中，便产生了一种新的进程同步工具-**管程(Monitors)**

代表共享资源的数据结构以及由对该共享数据结构实施操作的一组过程所组成的资源管理程序共同构成了一个操作系统的**资源管理模块**，我们称之为**管程**。包含四个部分：管程的名称、共享数据结构说明、对共享数据进行操作的一组过程、对共享数据设置初始值的语句

**特点**：

- 管程中包含了面向对象的思想，它将表征共享资源的数据结构及其对数据结构操作的一组过程，包括同步机制，都集中并封装在一个对象内部

- 封装于管程内部的数据结构仅能被封装于管程内部的过程所访问
- 在一个时刻只能有一个进程使用管程。进程在无法继续执行的时候不能一直占用管程，否则其它进程永远不能使用管程

<img src="Image\image-20210710180530431.png" alt="image-20210710180530431" style="zoom:50%;" />

描述如下：

```C
Monitor monitor_name {/*管程名*/
    share variable declarations;/*共享变量说明*/
	cond declarations;	/*条件变量说明*/
	public:				/*能被进程调用的过程*/
	void P1(......)		/*对数据结构操作的过程*/
	void P2(...)
	...
    {
        /*管程主体*/
        initialization code;
    }
}
```

**管程和进程的区别：**

- 数据结构：进程定义的是私有数据结构PCB,管程定义的是公共数据结构，如消息队列等
- 进程是由顺序程序执行有关操作，而管程主要是进行同步操作和初始化操作
- 设置进程的目的在于实现系统的并发性，而管程的设置则是解决共享资源的互斥使用问题
- 进程通过调用管程中的过程对共享数据结构实行操作，该过程就如通常的子程序一样被调用，因而管程为被动工作方式，进程则为主动工作方式
- 进程之间能并发执行，而管程则不能与其调用者并发
- 进程具有动态性，由“创建”而诞生，由“撤消”而消亡，而管程则是操作系统中的一个资源管理模块，供进程调用

**条件变量**

当一个进程调用了管程，同时该进程又被阻塞或挂起，如果该进程不释放管程，则其它进程无法进入管程，被迫长时间的等待。为了解决这个问题，引入了**条件变量condition**。在管程中设置多个条件变量，格式为：condition x，y;

- x.wait：正在调用管程的进程因 x 条件需要被阻塞或挂起，则调用x.wait将自己插入到x条件的等待队列上，并释放管程，直到x条件变化。此时其它进程可以使用该管程。
- x.signal：正在调用管程的进程发现x条件发生了变化，则调用x.signal重新启动一个因x条件而阻塞或挂起的进程，如果存在多个这样的进程，则选择其中的一个，如果没有，继续执行原进程，而不产生任何结果。与信号量机制中的signal操作不同，因为后者总是要执行s:=S+1操作，因而总会改变信号量的状态。

### 经典同步问题

**1.生产者消费者问题**

解决方法：

- 管程
- 信号量

**2 哲学家进餐问题**

五个哲学家围着一张圆桌，每个哲学家面前放着食物。哲学家的生活有两种交替活动：吃饭以及思考。当一个哲学家吃饭时，需要先拿起自己左右两边的两根筷子，并且一次只能拿起一根筷子。

<img src="Image\8"  style="zoom:80%;" />

下面是一种错误的解法，如果所有哲学家同时拿起左手边的筷子，那么所有哲学家都在等待其它哲学家吃完并释放自己手中的筷子，导致死锁。

```C
#define N 5

void philosopher(int i) {
    while(TRUE) {
        think();
        take(i);       // 拿起左边的筷子
        take((i+1)%N); // 拿起右边的筷子
        eat();
        put(i);
        put((i+1)%N);
    }
}
```

为了防止死锁的发生，可以设置两个条件：

- 必须同时拿起左右两根筷子；
- 只有在两个邻居都没有进餐的情况下才允许进餐。

**3. 读者-写者问题**

允许多个进程同时对数据进行读操作，但是不允许读和写以及写和写操作同时发生。

一个整型变量 count 记录在对数据进行读操作的进程数量，一个互斥量 count_mutex 用于对 count 加锁，一个互斥量 data_mutex 用于对读写的数据加锁。



## 进程通信【重要】

进程同步与进程通信很容易混淆，它们的区别在于：

- 进程同步：控制多个进程按一定顺序执行；
- 进程通信：进程间传输信息。

进程通信是一种手段，而进程同步是一种目的。也可以说，**为了能够达到进程同步的目的，需要让进程进行通信**，传输一些进程同步所需要的信息。

### 进程通信类型

#### 共享存储通信

- 共享数据结构：要求诸进程公用某些数据结构，以实现诸进程间的信息交换。这种通信方式仅适于传递相对少量的数据，通信效率低下，属于**低级通信**。
- 共享存储区：在内存中划出了一块共享存储区域，诸进程可通过对该共享区的读或写交换信息，实现通信，属于**高级通信**，可快速传输大量数据

**特点：**

- **需要使用信号量用来同步对共享存储的访问**
- 多个进程可以将同一个文件映射到它们的地址空间从而实现共享内存
-  XSI 共享内存不是使用文件，而是使用内存的匿名段。

#### 管道通信系统

> “管道”是指用于连接一个读进程和一个写进程以实现它们之间通信的一个**共享文件**，又名pipe文件。
>
> 向管道(共享文件)提供输入的发送进程(即写进程)以字符流形式将大量的数据送入管道，接受管道输出的接收进程(即读进程)则从管道中接收(读)数据，由于发送进程和接收进程是利用管道进行通信的，故又称为**管道通信**。

主要过程 => 管道机制进行协调

- 互斥，即当一个进程正在对pipe执行读/写操作时，其它(另一)进程必须等待。

- 同步，指当写(输入)进程把一定数量(如4 KB)的数据写入pipe，便去睡眠等待，直到读(输出)进程取走数据后再把它唤醒

  当读进程读一空pipe时，也应睡眠等待，直至写进程将数据写入管道后才将之唤醒。

- 确定对方是否存在，只有确定了对方已存在时才能进行通信

如图所示，通过调用 pipe 函数创建管道，fd[0] 用于读，fd[1] 用于写。

它具有以下限制：

- 只支持**半双工通信**（单向交替传输）
- **只能在父子进程或者兄弟进程中使用**

<img src="Image\9" alt="68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f35336364396164652d623061362d343339392d623464652d3766316662643036636466622e706e67" style="zoom:80%;" />

#### 命名管道

https://www.cnblogs.com/nufangrensheng/p/3561632.html

也称为 FIFO，也是一种文件类型，管道只能由相关进程使用，这些相关进程的共同祖先进程创建了管道。但是，通过FIFO，不相关的进程也能交换数据，**去除了管道只能在父子进程中使用的限制**。

FIFO 常用于客户-服务器应用程序中，FIFO 用作汇聚点，在客户进程和服务器进程之间传递数据。

> 如图：如果有一个服务器进程，它与很多客户进程有关，则每个客户进程都可将其请求写到一个该服务器进程创建的FIFO（所有客户进程都有该文件路径）中。因为对于该FIFO有多个写进程，客户进程发送给服务器进程的请求其长度要小于PIPE_BUF字节。这就能避免客户多个write之间的交错。

<img src="Image\11"   style="zoom:80%;" />

#### 消息传递系统

进程不必借助任何共享存储区或数据结构，而是以格式化的消息(message)为单位，将通信的数据封装在消息中，并利用操作系统提供的一组通信命令(原语)，在进程间进行消息传递，完成进程间的数据交换。例如计算机网络中的报文。

具体又可以分为直接通信方式和间接通信方式

**客户机-服务器系统的通信 **有如下几类

####  套接字

**套接字是进程通信和网络通信的基本构件**，为客户/服务器模型而设计的

一个套接字就是一个通信标识类型的数据结构，包含了

- 通信目的的**地址**
- 通信使用的**端口号**
- 通信网络的**传输层协议**
- 进程所在的**网络地址**
- 以及针对客户或服务器程序提供的不同系统调用(或API函数)等。

通常，套接字包括两类:

- 基于文件型：通信进程都运行在同一台机器的环境中，套接字是基于本地文件系统支持的，一个套接字关联到一个特殊的文件，通信双方通过对这个特殊文件的读写实现通信，其原理类似于前面所讲的管道。

- **基于网络型**：该类型通常采用的是非对称方式通信，即发送者需要提供接收者命名。通信双方的进程运行在不同主机的网络环境下，被分配了一对套接字，一个属于**接收进程**，一个属于**发送进程**

  > 发送进程(或客户端)发出连接请求时，随机申请一个套接字，主机为之分配一个端口，与该套接字绑定，不再分配给其它进程。
  >
  > 接收进程(或服务器端)拥有**全局公认的套接字和指定的端口**(如ftp服务器监听端口为21， Web或http服务器监听端口为80)，并通过监听端口等待客户请求。因此，任何进程都可以向它发出连接请求和信息请求，以方便进程之间通信连接的建立。
  >
  > 接收进程(或服务器端)一旦收到请求，就接受来自发送进程(或客户端)的连接，完成连接，即在主机间传输的数据可以准确地发送到通信进程，实现进程间的通信;当通信结束时，系统通过关闭接收进程(或服务器端)的套接字撤销连接。

套接字的优势：

- 不仅**适用于同一台计算机内部的进程通信**，也**适用于网络环境中不同计算机间的进程通信**
- 由于每个套接字拥有唯一的套接字号(也称套接字标识符)，这样系统中**所有的连接都持有唯一的一对套接字及端口连接**，对于来自不同应用程序进程或网络连接的通信，能够方便地加以区分，确保了通信双方之间**逻辑链路的唯一性**，便于实现数据传输的并发服务
- 隐藏了通信设施及实现细节，采用统一的接口进行处理

#### RPC

远程过程调用RPC(Remote Procedure Call)，是一个**通信协议**，用于通过网络连接的系统。该协议允许运行于一台主机(本地)系统上的进程调用另一台主机(远程)系统上的进程，而对程序员表现为常规的**过程调用**，无需额外地为此编程。

如果涉及的软件采用面向对象编程，那么远程过程调用亦可称做远程方法调用。负责处理远程过程调用的进程有两个，一个是本地客户进程，另一个是远程服务器进程，这两个进程通常也被称为**网络守护进程**，主要负责在网络间的消息传递，一般情况下，这两个进程都是处于阻塞状态，等待消息。

**基本概念**

https://www.jianshu.com/p/7d6853140e13

- RPC（Remote Procedure Call）远程过程调用，简单的理解是一个节点请求另一个节点提供的服务
- 本地过程调用：如果需要将本地student对象的age+1，可以实现一个addAge()方法，将student对象传入，对年龄进行更新之后返回即可，本地方法调用的函数体通过函数指针来指定。
- 远程过程调用：上述操作的过程中，如果addAge()这个方法在服务端，执行函数的函数体在远程机器上，如何告诉机器需要调用这个方法呢？

具体过程

1. 首先客户端需要告诉服务器，需要调用的函数，这里函数和进程ID存在一个映射，客户端远程调用时，需要查一下函数，找到对应的ID，然后执行函数的代码
2. 客户端需要把本地参数传给远程函数，本地调用的过程中，直接压栈即可，但是在远程调用过程中不再同一个内存里，无法直接传递函数的参数，因此需要客户端把参数转换成字节流，传给服务端，然后服务端将字节流转换成自身能读取的格式，是一个序列化和反序列化的过程
3. 数据准备好了之后，如何进行传输？网络传输层需要把调用的ID和序列化后的参数传给服务端，然后把计算好的结果序列化传给客户端，因此TCP层即可完成上述过程，gRPC中采用的是HTTP2协议

![image-20210804012520548](Image\image-20210804012520548.png)

总结一下上述过程：

**Client端** 

`Student student = Call(ServerAddr, addAge, student)`

1. 将这个调用映射为Call ID。
2. 将Call ID，student（params）序列化，以二进制形式打包
3. 把2中得到的数据包发送给ServerAddr，这需要使用网络传输层
4. 等待服务器返回结果
5. 如果服务器调用成功，那么就将结果反序列化，并赋给student，年龄更新

**Server端**

1. 在本地维护一个Call ID到函数指针的映射call_id_map，可以用Map<String, Method> callIdMap
2. 等待客户端请求
3. 得到一个请求后，将其数据包反序列化，得到Call ID
4. 通过在callIdMap中查找，得到相应的函数指针
5. 将student（params）反序列化后，在本地调用addAge()函数，得到结果
6. 将student结果序列化后通过网络返回给Client

在微服务的设计中，一个服务A如果访问另一个Module下的服务B，可以采用HTTP REST传输数据，并在两个服务之间进行序列化和反序列化操作，服务B把执行结果返回过来。

> 由于HTTP在应用层中完成，整个通信的代价较高，远程过程调用中直接基于TCP进行远程调用，数据传输在传输层TCP层完成，更适合对效率要求比较高的场景，RPC主要依赖于客户端和服务端之间建立Socket链接进行，底层实现比REST更复杂

![image-20210804012622753](Image\image-20210804012622753.png)

**一些问题：**

https://developer.51cto.com/art/201906/597963.htm

**基于 TCP 和基于 HTTP 协议的RPC有什么区别？**

基于 TCP 的协议实现的 RPC 调用，由于 TCP 协议处于协议栈的下层，能够更加灵活地对协议字段进行定制，减少网络开销，提高性能，实现更大的吞吐量和并发数。

基于 HTTP 协议实现的 RPC 则可以使用 JSON 和 XML 格式的请求或响应数据。

而 JSON 和 XML 作为通用的格式标准(使用 HTTP 协议也需要序列化和反序列化，不过这不是该协议下关心的内容，成熟的 Web 程序已经做好了序列化内容)，开源的解析工具已经相当成熟，在其上进行二次开发会非常便捷和简单。

但是由于 HTTP 协议是上层协议，发送包含同等内容的信息，使用 HTTP 协议传输所占用的字节数会比使用 TCP 协议传输所占用的字节数更高。

因此在同等网络下，通过 HTTP 协议传输相同内容，效率会比基于 TCP 协议的数据效率要低，信息传输所占用的时间也会更长，当然压缩数据，能够缩小这一差距。







**消息传递通信的实现方式**

> 在进程之间通信时，源进程可以直接或间接地将消息传送给目标进程，因此可将进程通信分为**直接和间接**两种通信方式。常见的直接消息传递系统和信箱通信就是分别采用这两种通信方式。

### 消息传递通信

**1.直接消息传递**

发送进程利用OS所提供的发送命令(原语)，直接把消息发送给目标进程。

**2.信箱通信**

信箱通信属于间接通信方式，即进程之间的通信，需要通过某种中间实体(如共享数据结构等)来完成。该实体建立在随机存储器的公用缓冲区上，用来暂存发送进程发送给目标进程的消息

如图2-17所示，

- 把待发送的消息正文、发送进程标识符、消息长度等信息填入其中

- 然后调用发送原语，把消息发送给目标(接收)进程

- 发送原语首先根据发送区 a中所设置的消息长度a.size来申请一缓冲区i，接着，把发送区 a中的信息复制到缓冲区i中。

- 为了能将i挂在接收进程的消息队列mq 上，应先获得接收进程的内部标识符j，然后将i挂在j.mq上

  > 该队列属于临界资源，故在执行insert 操作的前后都要执行wait和 signal操作

<img src="Image\image-20210711000123555.png" alt="image-20210711000123555" style="zoom: 80%;" />

## 线程

线程是独立调度和分派的基本单位。

**一个进程中可以有多个线程，它们共享进程资源**。

QQ 和浏览器是两个进程，浏览器进程里面有很多线程，例如 HTTP 请求线程、事件响应线程、渲染线程等等，线程的并发执行使得在浏览器中点击一个新链接从而发起 HTTP 请求时，浏览器还可以响应用户的其它事件。

> window 一个进程最多可以开2048个线程

<img src="Image\image-20210615194341075.png" alt="image-20210615194341075" style="zoom:67%;" />

**进程和线程的区别**

1. 调度

   线程是独立调度的基本单位，在**同一进程中，线程的切换不会引起进程切换**，从一个进程中的线程切换到另一个进程中的线程时，会引起进程切换。

2. 拥有资源

   进程是资源分配的基本单位，但是**线程不拥有系统资源**，只有一个保证独立运行的线程控制块TCB，**线程可以访问隶属进程的资源**

3. 独立性

   **同一进程中的不同线程之间的独立性要比不同进程之间的独立性低得多**。

   每个进程都拥有一个独立的地址空间和资源，不允许其它进程的访问。但是同一进程中的不同线程共享进程的内存地址空间和资源

   > 如一个线程的堆栈可以被其它线程读、写，甚至完全清除

4. 系统开销

   由于**创建或撤销进程**时，系统都要为之分配或回收资源，如内存空间、I/O 设备等，**所付出的开销远大于创建或撤销线程时的开销**。类似地，在进行进程切换时，涉及当前执行进程 CPU 环境的保存及新调度进程 CPU 环境的设置，而**线程切换时只需保存和设置少量寄存器内容，开销很小**。

5. 通信方面

   线程间可以通过直接**读写同一进程**中的数据进行通信，但是进程通信需要借助 IPC（Inter-Process Communication，进程间通信）。

**线程控制块 TCB**

线程控制块通常有这样几项:

- 线程标识符，为每个线程赋予一个唯一的线程标识符
- 一组寄存器，包括程序计数器PC、状态寄存器和通用寄存器的内容
- 线程运行状态，用于描述线程正处于何种运行状态
- 优先级，描述线程执行的优先程度
- 线程专有存储区，用于线程切换时存放现场保护信息，和与该线程相关的统计信息等
- 信号屏蔽，即对某些信号加以屏蔽
- 堆栈指针，在线程运行时，经常会进行过程调用，而过程的调用通常会出现多重嵌套的情况，这样，就必须将每次过程调用中所使用的局部变量以及返回地址保存起来

## 线程的实现方式

**1. 内核支持线程KST(Kernel Supported Threads)**

无论是系统进程还是用户进程，都是在操作系统内核的支持下运行的，创建、阻塞、撤消和切换等，也都是在内核空间实现的。

**2. 用户级线程ULT(User Level Threads)**

用户级线程是在用户空间中实现的。对线程的创建、撤消、同步与通信等功能，都无需内核的支持，即用户级线程是与内核无关的

**3. 组合线程**

把用户级线程和内核支持线程两种方式进行组合，提供了组合方式ULT/KST线程。

支持多个内核支持线程的建立、调度和管理，同时，也允许用户应用程序建立、调度和管理用户级线程。由于用户级线程和内核支持线程连接方式的不同，从而形成了三种不同的模型:多对一模型、一对一模型和多对多模型:

<img src="Image\image-20210711001558371.png" alt="image-20210711001558371" style="zoom:80%;" />

# 进程的调度

不同环境的调度算法目标不同，因此需要针对不同环境来讨论调度算法。

## 基本概念

CPU调度算法的共同目标：提高资源利用率，保证公平性、平衡性、策略强制执行。

CPU利用率公式如下
$$
CPU利用率=\frac{CPU 有效工作时间}{CPU有效工作时间+CPU空闲等待时间}
$$


**1.作业和作业步**

- 作业(Job)。作业是一个比程序更为广泛的概念，它不仅**包含了通常的程序和数据**，而且还应**配有一份作业说明书**，系统根据该说明书来对程序的运行进行控制。在批处理系统中，是以作业为基本单位从外存调入内存的。

- 作业步(Job Step)，通常，在作业运行期间，每个作业都必须经过若干个相对独立，又相互关联的顺序加工步骤才能得到结果。我们把其中的每一个加工步骤称为一个作业步，各作业步之间存在着相互联系，往往是上一个作业步的输出作为下一个作业步的输入。例如，一个典型的作业可分成: “编译”作业步， “链接装配”作业步和“运行”作业步。

**2. 作业控制块(Job Control Block， JCB)**

为了管理和调度作业，在多道批处理系统中，为每个作业设置了一个作业控制块JCB，它是作业在系统中存在的标志，其中保存了系统对作业进行**管理和调度所需的全部信息**。包括：作业标识、用户名称、作业状态、调度信息等。

## 作业调度算法

作业调度的主要任务是，根据 JCB 中的信息，检查系统中的资源能否满足作业对资源的需求，以及按照一定的调度算法，从外存的后备队列中选取某些作业调入内存，并为它们创建进程、分配必要的资源，然后再将新创建的进程排在就绪队列上等待调度。主要有以下几种

**1 先来先服务 **

first-come first-serverd，FCFS是最简单的调度算法，该算法既可用于作业调度，也可用于进程调度。按照请求的顺序进行调度。

有利于长作业，但不利于短作业，因为短作业必须一直等待前面的长作业执行完毕才能执行，而长作业又需要执行很长时间，造成了短作业等待时间过长。

**2 短作业优先**

shortest job first，SJF 是非抢占式的调度算法，按估计运行时间最短的顺序进行调度。

长作业有可能会饿死，处于一直等待短作业执行完毕的状态。因为如果一直有短作业到来，那么长作业永远得不到调度。

**3 最短剩余时间优先 **

shortest remaining time next，SRTN 是最短作业优先的抢占式版本，按剩余运行时间的顺序进行调度。 当一个新的作业到达时，其整个运行时间与当前进程的剩余时间作比较。如果新的进程需要的时间更少，则挂起当前进程，运行新的进程。否则新的进程等待。

**4 优先级调度**

由外部赋予作业相应的优先级，调度算法是根据该优先级进行调度的

**5 高响应比优先调度算法**

(Highest Response Ratio Next，HRRN) 既考虑了作业的等待时间，又考虑作业运行时间的调度算法，因此既照顾了短作业，又不致使长作业的等待时间过长，从而改善了处理机调度的性能

## 进程调度机制

进程调度的任务主要有：

- 保存处理机的现场信息。在进行调度时首先需要**保存当前进程**的处理机的现场信息，如程序计数器、多个通用寄存器中的内容等。
- 按某种算法**选取进程**。调度程序按某种算法从就绪队列中选取一个进程，将其状·态改为运行状态，并准备把处理机分配给它。
- 把处理器**分配给进程**。由分派程序把处理器分配给该进程，此时需要将选中进程的进程控制块内有关处理机现场的信息装入处理器相应的各个寄存器中，把处理器的控制权交予该进程，让它从上次的断点处恢复运行。

如下图所示，为了实现进程调度，在进程调度机制中，应具有如下三个基本部分

- **排队器**。为了提高进程调度的效率，应事先将系统中的所有就绪进程按照一定的策略排成一个或多个队列，以便调度程序能最快地找到它。以后每当有一个进程转变为就绪状态时，排队器便将它插入到相应的就绪队列。
- **分派器**。分派器依据进程调度程序所选定的进程，将其从就绪队列中取出，然后进行从分派器到新选出进程间的上下文切换，将处理机分配给新选出的进程。
- **上下文切换器**。在对 CPU 进行切换时，会发生两对上下文的切换操作
  - 一、OS 将保存当前进程的上下文，即把当前进程的 CPU **寄存器内容保存到该进程的进程控制块**内的相应单元，再装入分派程序的上下文，以便分派程序运行
  - 二、移出分派程序的上下文，而把新选进程的CPU现场信息装入到 CPU 的各个相应寄存器中，以便新选进程运行。

<img src="Image\image-20210620212018444.png" alt="image-20210620212018444" style="zoom: 50%;" />

进程调度方式：

- **非抢占方式**

  一旦把处理机分配给就绪队列中优先级最高的进程后，该进程便一直执行下去直至完成，或者因该进程发生某事件而放弃处理机时，系统方可将处理机重新分配给另一优先级最高的进程

- **抢占方式**

  把处理机分配给优先级最高的进程，使之执行。但在其执行期间，只要出现了另一个其优先级更高的进程，调度程序就将处理机分配给新到的优先级最高的进程。

### 轮转调度算法

在分时系统中，最简单也是较常用的是**基于时间片的轮转**(round robin，RR)调度算法。该算法采取了非常公平的处理机分配方式，即让就绪队列上的每个进程每次仅运行一个时间片

> 如果就绪队列上有n个进程，则每个进程每次大约都可获得1/n的处理机时间

**基本原理**

将所有就绪进程按 FCFS 的原则排成一个队列，每次调度时，把 CPU 时间分配给队首进程，该进程可以**执行一个时间片**。当时间片用完时，由计时器发出时钟中断，调度程序便停止该进程的执行，并将它**送往就绪队列的末尾**，同时继续把 CPU 时间分配给队首的进程。

时间片轮转算法的效率和时间片的大小有很大关系：

- 因为进程切换都要保存进程的信息并且载入新进程的信息，如果时间片太小，会导致进程切换得太频繁，在进程切换上就会花过多时间。
- 而如果时间片过长，那么实时性就不能得到保证。

<img src="Image\6" alt="68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f38633636323939392d633136632d343831632d396634302d3166646261356263393136372e706e67"  />

### 优先级调度算法

为每个进程分配一个优先级，按优先级进行调度。

为了防止低优先级的进程永远等不到调度，可以**随着时间的推移增加等待进程的优先级**。

### 多级队列调度算法

将系统中的进程就绪队列从一个拆分为若干个，将不同类型或性质的进程固定分配在不同的就绪队列，不同的就绪队列采用不同的调度算法，一个就绪队列中的进程可以设置不同的优先级，不同的就绪队列本身也可以设置不同的优先级。

### 多级反馈队列算法

一个进程需要执行 100 个时间片，如果采用时间片轮转调度算法，那么需要交换 100 次。

**多级队列**是为这种需要连续执行多个时间片的进程考虑，它设置了多个队列，每个队列时间片大小都不同，例如 1 2 4 8 16 32 64..。进程在第一个队列没执行完，就会被移到下一个队列。这种方式下，之前的进程只需要交换 7 次。

每个队列优先权也不同，**最上面的优先权最高**。因此只有上一个队列没有进程在排队，才能调度当前队列上的进程。

可以将这种调度算法看成是时间片轮转调度算法和优先级调度算法的结合。

<img src="Image\7" alt="68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f30343263663932382d336338652d343831352d616539632d6632373830323032633638662e706e67" style="zoom: 80%;" />

### 基于公平原则的调度算法

包括保证调度算法和公平分享调度算法



## 实时系统的调度

实时系统要求一个请求在一个确定时间内得到响应。分为硬实时和软实时，前者必须满足绝对的截止时间，后者可以容忍一定的超时。

实时系统的调度算法主要有非抢占式调度算法和抢占式调度算法

### 最早截止时间优先算法

 EDF(Earliest Deadline First)

该算法是根据任务的截止时间确定任务的优先级，任务的截止时间愈早，其优先级愈高，具有最早截止时间的任务排在队列的队首。

### 最低松弛度优先算法

LLF(Least Laxity First)

该算法在确定任务的优先级时，根据的是任务的紧急(或松弛)程度。任务紧急程度愈高，赋予该任务的优先级就愈高，以使之优先执行

### 优先级倒置

(priority inversion problem)

当前OS 广泛采用优先级调度算法和抢占方式，然而在系统中存在着影响进程运行的资源而可能产生“优先级倒置”的现象，即高优先级进程(或线程)被低优先级进程(或线程)延迟或阻塞

假如有三个完全独立的进程P1、P2和P3，P1的优先级最高，P2次之，P3最低。P1和P3通过共享的一个临界资源进行交互。如图3-10所示

- 假如P3最先执行，在执行了P(mutex)操作后，进入到临界区 CS-3。在时刻a，P2就绪，因为它比P3的优先级高，P2抢占了P3的处理机而运行
- 在时刻b，P1就绪，因为它又比P2的优先级高，P1抢占了P2的处理机而运行
- 在时刻c，P2执行P(mutex)操作，试图进入临界区CS-1，但因为相应的临界资源已被P3占用，故P1将被阻塞。
- 由P2继续运行，直到时刻d运行结束。然后由P3接着运行，到时刻e时P3退出临界区，并唤醒P1。因为它比P3的优先级高，故它抢占了P3的处理机而运行

<img src="Image\image-20210711004335987.png" alt="image-20210711004335987" style="zoom:80%;" />

根据优先级原则，高优先级进程应当能优先执行，但在此例中，P1和P3共享着“临界资源”，而出现了不合常理的现象，高优先级进程P1因P3进程被阻塞了，又因为P2进程的存在而延长了P1被阻塞的时间，这是有害的

**解决办法**

建立在动态优先级继承基础上的。如下图所示

- 当高优先级进程P1要进入临界区，去使用临界资源R，如果已有一个低优先级进程P3正在使用该资源，此时一方面P1被阻塞，另一方面由P3继承P1的优先级，并一直保持到P3退出临界区。

  <img src="Image\image-20210711004651274.png" alt="image-20210711004651274" style="zoom:80%;" />

# 死锁

> 系统中只有一台扫描仪R，和一台刻录机R2，有两个进程P，和P2，它们都准备将扫描的文挡刻录到CD光盘上，进程P，先请求扫描仪R，并获得成功，进程P2先请求CD刻录机R2也获得成功。后来P，又请求CD刻录机，因它已被分配给了P2而阻塞。P2又请求扫描仪，也因被分配给了P，而阻塞，此时两个进程都被阻塞，双方都希望对方能释放出自己所需要的资源，但它们谁都因不能获得自己所需的资源去继续运行，从而无法释放出自己占有的资源，并且**一直处于这样的僵持状态而形成死锁**

定义：如果一组进程中的每一个进程都在等待仅由该组进程中的其它进程才能引发的事件，那么该组进程是死锁的(Deadlock)。

## 一些概念

**1.可重用性资源和消耗性资源**

**可重用性资源**是一种可供用户重复使用多次的资源，有以下特点：

- 每一个可重用性资源中的单元只能分配给一个进程使用，不允许多个进程共享。
- 进程在使用可重用性资源时，须按照这样的顺序
  - 请求资源。如果请求资源失·败，请求进程将会被阻塞或循环等待
  - 使用资源。进程对资源进行操作，如用打印机进行打印
  - 释放资源。当进程使用完后自己释放资源

- 系统中每一类可重用性资源中的单元数目是相对固定的，进程在运行期间既不能创建也不能删除它。

**可消耗性资源**又称为临时性资源，它是在进程运行期间，由进程动态地创建和消耗的，它具有如下性质

- 每一类可消耗性资源的单元数目在进程运行期间是可以不断变化的，，有时它可以有许多，有时可能为0
- 进程在运行过程中，可以不断地创造可消耗性资源的单元，将它们放入该资源类的缓冲区中，以增加该资源类的单元数目
- 进程在运行过程中，可以请求若干个可消耗性资源单元，用于进程自己的消耗，不再将它们返回给该资源

**2. 可抢占性资源和不可抢占性资源**

**可抢占性资源** 指某进程在获得这类资源后，该资源可以再被其它进程或系统抢占。CPU和主存均属于可抢占性资源。对于这类资源是不会引起死锁的

**不可抢占性资源**即一旦系统把某资源分配给该进程后，就不能将它强行收回，只能在进程用完后自行释放。例如磁带机、打印机等。

## 引起死锁的原因

死锁的起因，通常是源于多个进程对资源的争夺，不仅对不可抢占资源进行争夺时会引起死锁，而且对可消耗资源进行争夺时，也会引起死锁

**1. 竞争不可抢占性资源引起死锁**

资源分配图：用方块代表可重用的资源(文件)，用圆圈代表进程

如图3-12所示。当箭头从进程指向文件时，表示进程请求资源(打开文件);当箭头从资源指向进程时，表示该资源已被分配给该进程(已被进程打开)。从中可以看出，这时在P1、P2及R1和R2之间，已经形成了一个环路，说明已进入死锁状态。

<img src="Image\34" alt="image-20210620214605317" style="zoom: 67%;" />

**2. 竞争可消耗资源引起死锁**

图3-13示出了在三个进程之间，在利用消息通信机制进行通信时所形成的死锁情况。图中， m1、m2和m3是可消耗资源。

进程P1一方面产生消息m1，利用send(p2， m1)原语将它发送给P2;另一方面，它又要求从P3接收消息m3。P2和P3类似，

如果按照以下方式，先发送，后接收，则不产生死锁

```
p1:  send(p2,m1); receive(p3,m3);
p2:  send(p3,m2); receive(p1,m1);
p3:  send(p1,m3); receive(p2,m2);
```

如果按照以下方式，先接收，后发送，则产生死锁

```
p1:  receive(p3,m3);send(p2,m1);
p2:  receive(p1,m1);send(p3,m2);
p3:  receive(p2,m2); send(p1,m3);
```

<img src="Image\image-20210620214621104.png" alt="image-20210620214621104" style="zoom: 67%;" />

**3. 进程推进顺序不当引起死锁**

若并发进程P1和P2按图3-14中曲线④所示的顺序推进，它们将进入不安全区D内。此时P1保持了资源R1， P2保持了资源R2，系统处于不安全状态。此刻，如果两个进程继续向前推进，就可能发生死锁。

> 例如，当P1，运行到P1: Request(R2)时，将因R2已被P2占用而阻塞;当P2运行到P2: Request(R1)时，也将因R1，已被P1，占用而阻塞，于是发生了进程死锁，这样的进程推进顺序就是非法的。

<img src="Image\image-20210620215154417.png" alt="image-20210620215154417" style="zoom: 50%;" />

## 产生死锁的必要条件【重要】

产生死锁必须**同时具备**下面四个必要条件，只要其中任一个条件不成立，死锁就不会发生：

- **互斥**条件：进程对所分配到的资源进行排它性使用，即在一段时间内，某资源只能被一个进程占用。如果此时还有其它进程请求该资源，则请求进程只能等待，直至占有"该资源的进程用毕释放。
- **请求和保持**条件：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程被阻塞，但对自己已获得的资源保持不放。
- **不可抢占**条件：进程已获得的资源在未使用完之前不能被抢占，只能在进程使用完时由自己释放。
- **循环等待**条件：在发生死锁时，必然存在一个进程一资源的循环链，即进程集合(P， P， P2， ..P)中的Po正在等待一个P，占用的资源， P，正在等待P2占用的资源.......P，正在等待已被Po占用的资源。

如进程 A 和 进程 B 对打印机和扫描仪进行调用，那么打印机和扫描仪被任意一个进程调用后就会锁定，不能再被调用了，这满足了**互斥条件**；接着，进程 A 占用了打印机，但是需要请求扫描仪，而进程 B 占有了扫描仪，又需要请求打印机，这满足了**请求和保持条件**；由于打印机、扫描仪是不可抢占资源，因此满足了**不可抢占条件**；显然，上述过程满足**循环等待条件**；四个条件都**同时满足**了，便发生死锁。

## 处理死锁的方法

目前处理死锁的方法可归结为四种

1. **预防死锁**。这是一种较简单和直观的事先预防方法。该方法是通过设置某些限制条件，去破坏产生死锁四个必要条件中的一个或几个来预防产生死锁。预防死锁是一种较易实现的方法，已被广泛使用
2. **避免死锁**。同样是属于事先预防策略，但它并不是事先采取各种限制措施，去破坏产生死锁的四个必要条件，而是在资源的动态分配过程中，用某种方法防止系统进入不安全状态，从而可以避免发生死锁
3. **检测死锁**。这种方法无须事先采取任何限制性措施，允许进程在运行过程中发生死锁。但可通过检测机构及时地检测出死锁的发生，然后采取适当的措施，把进程从死锁中解脱出来
4. **解除死锁**。当检测到系统中已发生死锁时，就采取相应措施，将进程从死锁状态中解脱出来。常用的方法是撤消一些进程，回收它们的资源，将它们分配给已处于阻塞状态的进程，使其能继续运行

### 预防死锁

在程序运行之前预防发生死锁。

预防死锁的方法是通过破坏产生死锁的四个必要条件中的一个或几个，以避免发生死锁。由于互斥条件是非共享设备所必须的，不仅不能改变，还应加以保证，因此主要是破**坏产生死锁的后三个条件**。

**1. 破坏请求和保持条件**

方式一：进程在开始执行前就获得所需要的全部资源，这样在进程的运行期间便不会提出资源请求

方式二：获得初期需要的资源，然后运行期间逐步释放这些资源

**2. 破坏不可抢占条件**

当某个不可抢占资源被占有时，拒绝其他进程提出的资源请求

**3. 破坏环路等待**

给资源统一编号，进程只能按编号顺序来请求资源。

### 避免死锁

在程序运行时避免发生死锁。

> 在死锁避免方法中，把系统的状态分为**安全状态和不安全状态**。当系统处于安全状态时，可避免发生死锁。反之，当系统处于不安全状态时，则可能进入到死锁状态。

**安全状态**：如果没有死锁发生，并且即使所有进程突然**请求对资源的最大需求**，也**仍然存在某种调度次序能够使得每一个进程运行完毕**，则称该状态是安全的。

例如，图 a 的第二列 Has 表示已拥有的资源数，第三列 Max 表示总共需要的资源数，Free 表示还有可以使用的资源数。从图 a 开始出发，先让 B 拥有所需的所有资源（图 b），运行结束后释放 B，此时 Free 变为 5（图 c）；接着以同样的方式运行 C 和 A，使得所有进程都能成功运行，因此可以称图 a 所示的状态时安全的。

![68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f65643532333035312d363038662d346333662d623334332d3338336532643139343437302e706e67](Image\15)



避免死锁的一些算法：**银行家算法**，具体分为以下两类

**1. 单个资源的银行家算法**

一个小城镇的银行家，他向一群客户分别承诺了一定的贷款额度，算法要做的是判断对请求的满足是否会进入不安全状态，如果是，就拒绝请求；否则予以分配。图 c 为不安全状态，因此算法会拒绝之前的请求，从而避免进入图 c 中的状态。

![68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f64313630656332652d636665322d343634302d626461372d3632663533653538623863302e706e67](Image\16)

**2. 多个资源的银行家算法**

图中有五个进程，四个资源。左边的图表示已经分配的资源，右边的图表示还需要分配的资源。最右边的 E、P 以及 A 分别表示：总资源、已分配资源以及可用资源，注意这三个为向量，而不是具体数值，例如 A=(1020)，表示 4 个资源分别还剩下 1/0/2/0。

检查一个状态是否安全的算法如下：

- 查找右边的矩阵是否存在一行小于等于向量 A。如果不存在这样的行，那么系统将会发生死锁，状态是不安全的。
- 假若找到这样一行，将该进程标记为终止，并将其已分配资源加到 A 中。
- 重复以上两步，直到所有进程都标记为终止，则状态时安全的。

如果一个状态不是安全的，需要拒绝进入这个状态。

![68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f36326530646434662d343463332d343365652d626236652d6665646239653036383531392e706e67](Image\17)

### 死锁的检测与解除

不试图阻止死锁，而是当检测到死锁发生时，采取措施进行恢复。

**死锁检测有2种法**：

**1. 检测环**

每种类型一个资源的死锁检测算法是通过检测**有向图**是否存在环来实现，从一个节点出发进行**深度优先搜索**，对访问过的节点进行标记，如果访问了已经标记的节点，就表示有向图存在环，也就是检测到死锁的发生。

例如：图 a 可以抽取出环（图 b），它满足了环路等待条件，因此会发生死锁。

<img src="Image\13" alt="68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f62316661303435332d613462302d346561652d613335322d3438616363613866666637342e706e67" style="zoom:80%;" />

**2. 每种类型的资源占用有多个的死锁检测**

如图，有三个进程四个资源，每个数据代表的含义如下：

- E 向量：资源总量
- A 向量：资源剩余量
- C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量
- R 矩阵：每个进程请求的资源数量

![68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f65316564613364352d356563382d343730382d386532352d3161303463356531316634382e706e67](Image\14)

进程 P1 和 P2 所请求的资源都得不到满足，**只有进程 P3 可以**，让 P3 执行，之后释放 P3 拥有的资源，此时 A = (2 2 2 0)。P2 可以执行，执行后释放 P2 拥有的资源，A = (4 2 2 1) 。P1 也可以执行。所有进程都可以顺利执行，没有死锁。

算法总结如下：

每个进程最开始时都不被标记，执行过程有可能被标记。当算法结束时，任何没有被标记的进程都是死锁进程。

1. 寻找一个没有标记的进程 Pi，它所请求的资源小于等于 A。
2. 如果找到了这样一个进程，那么将 C 矩阵的第 i 行向量加到 A 中，标记该进程，并转回 1。
3. 如果没有这样一个进程，算法终止。

**死锁的解除**

- 终止所有死锁进程

- 逐个终止进程直到打破循环等待
- 从一个或多个进程抢占资源并分配给死锁进程

# 存储器管理

## 存储器

计算机存储层次

- 最高层为CPU寄存器
- 中间为主存
- 底层是辅存

特点：固定磁盘和可移动存储介质属于**设备管理**，信息长期保存；磁盘缓存、主存储器（内存）、高速缓存、寄存器属于**OS管理**，掉电后丢失信息

<img src="Image\image-20210621102615059.png" alt="image-20210621102615059" style="zoom:50%;" />

### 主存储器

主存储器简称**内存**或主存，是计算机系统中的主要部件，用于**保存进程运行时的程序和数据**，也称可执行存储器。

通常，处理机都是从主存储器中**取得指令和数据**的，并将其所取得的指令放入指令寄存器中，而将其所读取的数据装入到数据寄存器中;或者反之，将寄存器中的数据存入到主存储器。

主存储器访问速度远低于CPU执行指令的速度，为缓和这一矛盾，在计算机系统中引入了寄存器和高速缓存。

### 寄存器

寄存器具有与处理机相同的速度，故对寄存器的访问速度最快，完全能与CPU协调工作，但价格却十分昂贵，因此容量不可能做得很大。在早期计算机中，寄存器的数目仅为几个，主要**用于存放处理机运行时的数据**，以加速存储器的访问速度，如**使用寄存器存放操作数，或用作地址寄存器加快地址转换速度**等。

### 高速缓存

高速缓存是现代计算机结构中的一个重要部件，它是介于寄存器和存储器之间的存储器，主要**用于备份主存中较常用的数据**，以减少处理机对主存储器的访问次数，这样可大幅度地提高程序执行速度。

### 磁盘缓存

由于目前磁盘的 IO 速度远低于对主存的访问速度，为了缓和两者之间在速度上的不匹配，而设置了磁盘缓存，主要**用于暂时存放频繁使用的一部分磁盘数据和信息**，以减少访问磁盘的次数。

但磁盘缓存与高速缓存不同，它本身并不是一种实际存在的存储器，而是利用主存中的部分存储空间暂时存放从磁盘中读出(或写入)的信息。

主存也可以看作是辅存的高速缓存，因为，辅存中的数据必须复制到主存方能使用，反之，数据也必须先存在主存中，才能输出到辅存。



## 程序的链接和装入【重要】

用户程序要在系统中运行，必须先将它装入内存，然后再将其转变为一个可以执行的程序，通常都要经过以下几个步骤

1. 编译：由**编译程序**对用户源程序进行编译，形成若干个目标模块
2. 链接：由**链接程序**将编译后形成的一组目标模块以及它们所需要的库函数链接在一起，形成一个完整的装入模块
3. 装入：由**装入程序**将装入模块装入内存

<img src="Image\image-20210621103457996.png" alt="image-20210621103457996" style="zoom: 50%;" />



### 链接

在对目标模块进行链接时，根据进行链接的时间不同，可把链接分成如下三种。

1. 静态链接(Static Linking)方式

   在程序运行之前，先将各目标模块及它们所需的库函数链接成一个完整的装配模块，以后不再拆开。我们把这种**事先进行链接的方式称为静态链接**方式。

2. 装入时动态链接(Load-time Dynamic Linking)

   指将用户源程序编译后所得到的一组目标模块，在装入内存时，采用边装入边链接的链接方式。即在装入一个目标模块时，若发生一个外部模块调用事件，将引起装入程序去找出相应的外部目标模块，并将它装入内存。装入时动态链接方式有以下优点：便于修改和更新、便于实现对目标模块的共享

3. 运行时动态链接(Run-time Dynamic Linking)

   将对某些模块的链接推迟到程序执行时才进行。亦即，在执行过程中，当发，现一个被调用模块尚未装入内存时，立即由os去找到该模块，并将之装入内存，**将其链接到调用者模块**上。凡在执行过程中未被用到的目标模块，都不会被调入内存和被链接到装入模块上，这样不仅能**加快程序的装入过程，而且可节省大量的内存空间**。



例如：以下是一个 hello.c 程序：

```C
#include <stdio.h>

int main()
{
    printf("hello， world\n");
    return 0;
}

```

在 Unix 系统上，由编译器把源文件转换为目标文件。

```bash
gcc -o hello hello.c
```

过程大致如下

- 预处理阶段：处理以 # 开头的预处理命令；
- 编译阶段：翻译成汇编文件；
- 汇编阶段：将汇编文件翻译成可重定位目标文件；
- 链接阶段：将可重定位目标文件和 printf.o 等单独预编译好的目标文件进行合并，得到最终的可执行目标文件。

<img src="Image\28" alt="68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f62333936643732362d623735662d346133322d383961322d3033613762366531396636662e6a7067" style="zoom:80%;" />

**该文件的静态链接**

静态链接器以一组可重定位目标文件为输入，生成一个完全链接的可执行目标文件作为输出。链接器主要完成以下两个任务：

- 符号解析：每个符号对应于一个函数、一个全局变量或一个静态变量，符号解析的目的是将每个符号引用与一个符号定义关联起来。
- 重定位：链接器通过把每个符号定义与一个内存位置关联起来，然后修改所有对这些符号的引用，使得它们指向这个内存位置。

<img src="Image\29" alt="68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f34376439383538332d386262302d343563632d383132642d3437656566613061346134302e6a7067" style="zoom:80%;" />

### 装入

在将一个装入模块装入内存时，可以有如下三种装入方式:

1. **绝对装入方式**(Absolute Loading Mode)

   用户程序经编译后，将产生绝对地址(即物理地址)的目标代码，此时可以采用绝对装入方式，只适用于单道程序环境

2. **可重定位装入方式**(Relocation Loading Mode)

   所谓**重定位**是指在装入时将指令和数据地址进行修改的过程

   在多道程序环境下，编译程序不可能预知经编译后所得到的目标模块应放在内存的何处。因此，对于用户程序编译所形成的若干个目标模块，它们的起始地址通常都是从0开始的，程序中的其它地址也都是相对于起始地址计算的。此时，不可能再用绝对装入方式，而应采用可重定位装入方式，其装入模块中的**所有逻辑地址与实际装入内存后的物理地址**不同

   例如：左边作业1000号单元的功能是取出2500处的整数365并放于寄存器，但是将该作业加载到内存中的10000~15000单元后，如果不进行地址变换，那么依然会取出2500处单元的值，这就导致数据错误，为此需要将指令中的逻辑2500与起始地址10000相加，变为125000

<img src="Image\image-20210621104524656.png" alt="image-20210621104524656" style="zoom:50%;" />

​		该重定位需要有一个重定位寄存器进行支持，如图所示，增设的重定位寄存器用来存放程序(数据)在内存中的起始地址。程序在执行时，真正访问的内存地址是相对地址与重定位寄存器中的地址相加而形成的。图4-12示出了动态重定位的实现原理。地址变换过程是在程序执行期间，随着对每条指令或数据的访问自动进行的，故称为动态重定位。

<img src="Image\456" alt="image-20210621111214519" style="zoom:50%;" />

3. **动态运行时的装入方式**(Dynamic Run-time Loading)

   可重定位装入方式不允许程序运行时在内存中移动位置。而实际情况是，在运行过程中程序在内存中的位置可能经常要改变。

   动态运行时的装入程序在把装入模块装入内存后，并不立即把装入模块中的逻辑地址转换为物理地址，而是**把这种地址转换推迟到程序真正要执行时才进行**。因此，装入内存后的所有地址都仍是逻辑地址。为使地址转换不影响指令的执行速度，这种方式需要一个重定位寄存器的支持

## 连续分配存储

连续分配方式指为一个用户程序分配一个连续的内存空间，即程序中**代码或数据的逻辑地址相邻**，体现在内存空间分配时物理地址的相邻。连续分配方式可分为四类:单一连续分配、固定分区分配、动态分区分配以及动态可重定位分区分配算法四种方式。

### 单一连续分配

在单道程序环境下，当时的存储器管理方式是把内存分为系统区和用户区两部分，系统区仅提供给OS使用，它通常是放在内存的低址部分。而在用户区内存中，仅装有一道用户程序，即整个内存的用户空间由该程序独占。这样的存储器分配方式被称为单一连续分配方式。

### 固定分区分配

将分区按大小进行排队，并建立一张分区使用表，其中各表项包括每个分区的起始地址、大小及状态(是否已分配)，如图4-5所示。当有一用户程序要装入时，由内存分配程序依据用户程序的大小检索该表，从中找出一个能满足要求.的、尚未分配的分区，将之分配给该程序，然后将该表项中的状态置为“已分配"。若未找到大小足够的分区，则拒绝为该用户程序分配内存。

<img src="Image\31" alt="image-20210621110248574" style="zoom:50%;" />

### 动态分区分配

根据进程的实际需要，动态地为之**分配内存**空间，然后**回收内存**

动态分区的数据结构有以下两种形式

- **空闲分区表**，在系统中设置一张空闲分区表，用于记录每个空闲分区的情况。每个空闲分区占一个表目，表目中包括分区号、分区大小和分区始址等数据项，如图4-6所示
- **空闲分区链**。为了实现对空闲分区的分配和链接，在每个分区的起始部分设置一些用于控制分区分配的信息，以及用于链接各分区所用的前向指针，在分区尾部则设置一后向指针。**通过前、后向链接指针，可将所有的空闲分区链接成一个双向链**，如图4-7所示。为了检索方便，在分区尾部重复设置状态位和分区大小表目。当分区被分配出去以后，把状态位由"0"改为"1"，此时，前、后向指针已无意义。



<img src="Image\image-20210621110451074.png" alt="image-20210621110451074" style="zoom:50%;" />

**分配的操作过程：**

1. 分配内存

   系统应利用某种分配算法，从空闲分区链(表)中找到所需大小的分区。

<img src="Image\image-20210711005455141.png" alt="image-20210711005455141" style="zoom: 80%;" />

2. 回收内存

   当进程运行完毕释放内存时，系统根据回收区的首址，从空闲区链(表)中找到相应的插入点，有四种情况

- 回收区与插入点的前一个空闲分区F相邻接 => 将回收区与插入点的前一分区合并
- 回收分区与插入点的后一空闲分区F相邻接 => 将两分区合并，用回收区的首址作为新空闲区的首址
- 回收区同时与插入点的前、后两个分区邻接 => 将三个分区合并，使用F的表项和F1的首址，取消F2的表项
- 回收区既不与F邻接，又不与F邻接 => 为回收区单独建立一个新表项，填写回收区的首址和大小，并根据其首址插入到空闲链中的适当位置

![image-20210711011315489](Image\image-20210711011315489.png)

**动态分区分配算法**：为把一个新作业装入内存，须按照一定的分配算法，从空闲分区表或空闲分区链中选出一分区分配给该作业

1.基于顺序搜索的动态分区分配算法

- 首次适应算法
- 循环首次适应算法
- 最佳适应算法
- 最坏适应算法

2.基于索引搜索的动态分区分配算法

- 快速适应算法
- 伙伴系统
- 哈希算法

### 动态可重定位分区分配

**1. 紧凑**

当一台计算机运行了一段时间后，它的内存空间将会被分割成许多小的分区，而缺乏大的空闲空间。即使这些分散的许多小分区的容量总和大于要装入的程序，但由于这些分区不相邻接，也无法把该程序装入内存。

例如，图4-11(a)中示出了在内存中现有四个互不的小分区，它分别为10 KB. 30 KB. 14 KB和26KB，其总容量是80 KB.但如果现在有一个作业到达，要求获得40 KB的内存空间，由于必须为它分配一个连续空间，故此作业无法装入。这种不能被利用的小分区即是前已提及的“碎片”，或称为“零头”。

若想把大作业装入，可采用的一种方法是:将内存中的所有作业进行移动，使它们全都相邻接。这样，即可把原来分散的多个空闲小分区拼接成一个大分区，可将一个作业装入该区。这种**通过移动内存中作业的位置，把原来多个分散的小分区拼接成一个大分区的方法，称为“拼接”或“紧凑”**，见图4-11(b)。

<img src="Image\image-20210621110931960.png" alt="image-20210621110931960" style="zoom: 50%;" />

当系统对内存进行了“紧凑”，而使若干程序从内存的某处移至另一处时，不需对程序做任何修改，只要用该程序在内存的新起始地址去置换原来的起始地址即可。比如下图，只要把重定位寄存处的数据替换即可，该过程称为**动态重定位**

<img src="Image\32" alt="image-20210621111214519" style="zoom:50%;" />

## 离散分配存储【重要】

连续分配方式会形成许多“碎片”，虽然可通过“紧凑”方法将许多碎片拼接成可用的大块空间，但须为之付出很大开销。如果允许将一个进程直接分散地装入到许多不相邻接的分区中，便可充分地利用内存空间，而无须再进行“紧凑”。基于这一思想而产生了离散分配方式。根据在离散分配时所分配地址空间的基本单位的不同，又可将离散分配分为：**分页存储管理、分段存储管理、段页式存储管理**

**1. 对换**

**对换技术**也称为交换技术，是指把内存中暂时不能运行的进程或者暂时不用的程序和数据换出到外存上，以便腾出足够的内存空间，再把已具备运行条件的进程或进程所需要的程序和数据换入内存，可以直接提高处理机的利用率和系统的吞吐量

对换分为两种类型：整体对换（进程对换）和页面对换（分段兑换）

**2. 虚拟内存**

虚拟内存的目的是为了让物理内存扩充成更大的逻辑内存，从而让程序获得更多的可用内存。

为了更好的管理内存，**操作系统将内存抽象成地址空间**。**每个程序拥有自己的地址空间**，这个地址空间被分割成多个块，每一块称为一页。这些页被映射到物理内存，但不需要映射到连续的物理内存，也不需要所有页都必须在物理内存中。当程序引用到不在物理内存中的页时，由硬件执行必要的映射，将缺失的部分从虚拟内存中装入物理内存并重新执行失败的指令。

### 分页存储管理

> 将用户程序的地址空间分为若干个固定大小的区域，称为“页”或“页面”。典型的页面大小为1 KB，相应地，也将内存空间分为若干个物理块或页框(frame)，页和块的大小相同。这样可将用户程序的任一页放入在一物理块中，实现了离散分配。

#### 基本概念

**1. 页面和物理块**

分页存储管理将进程的逻辑地址空间分成若干个页，并为各页加以编号，从0开始，如第0页、第1页等。相应地，也把内存的物理地址空间分成若干个块，同样也为它们加以编号，如0#块、1#块等等。在为进程分配内存时，以块为单位，将进程中的若干个页分别装入到多个可以不相邻接的物理块中。页面的大小应选择适中，且页面大小应是2的幂，常为1 KB~8 KB.

**2. 分页地址结构**

分页地址结构也就是**逻辑地址**（参考上面的可重定位装入），包含两部分内容:前一部分为页号P，后一部分为位(偏)移量w，即页内地址。图中的地址长度为32位，其中0-11位为**页内地址（偏移量）**，即每页的大小为4 KB; 12~31位为**页号**，地址空间最多允许有1M页。

<img src="Image\image-20210621113116093.png" alt="image-20210621113116093" style="zoom:50%;" />

**3. 页表**

在分页系统中，允许将进程的各个页离散地存储在内存的任一物理块中，为保证进程仍然能够正确地运行，即能在内存中找到每个页面所对应的物理块，系统又为每个进程建立了一张**页面映像表**，简称页表。在进程地址空间内的所有页(0~n)，依次在页表中有一页表项，其中记录了相应页在内存中对应的物理块号，见图4-14的中间部分。在配置了页表后，进程执行时，通过查找该表，即可找到每页在内存中的物理块号。可见，**页表的作用是实现从页号到物理块号的地址映射**。

<img src="Image\image-20210621113245961.png" alt="image-20210621113245961" style="zoom: 50%;" />

**4. 地址变换机构**

为了能将用户地址空间中的逻辑地址转换为内存空间中的物理地址，在系统中必须设置地址变换机构。该机构的基本任务是**实现从逻辑地址到物理地址的转换**。

地址变换机构的任务实际上只是**将逻辑地址中的页号转换为内存中的物理块号**，又因为页面映射表的作用就是用于实现从页号到物理块号的变换，因此，地址变换任务是借助于页表来完成的。页表大多驻留在内存中，在系统中只设置一个**页表寄存器**PTR(Page-Table Register)，在其中存放页表在内存的始址和页表的长度。

**具体实现过程有2种方式：**

####  基本地址变换机构

如图4-15所示，完成从逻辑地址到物理地址的变换过程如下

（1）进程未执行时，**页表始址和页表长度**存放在本进程的PCB中

（2）当调度程序调度到某进程时，才将上面这两个数据（页表始址 + 页表长度）装入**页表寄存器**

（3）当进程要访问某个逻辑地址中的数据时，分页地址变换机构会自动地将相对地址分为**页号和页内地址**两部分，再**以页号为索引去检索页表**。若没在页表中没找到，则发生**缺页中断**

（4）如果页号大于或等于页表长度，则产生**地址越界中断**。若未出现越界中断，则将页表始址与页号和页表项长度的乘积相加，便得到该表项在页表中的位置，于是可从中得到该页的物理块号，将之装入**物理地址寄存器**中

（5）将逻辑地址寄存器中的**页内地址**送入物理地址寄存器的块内地址字段中

<img src="Image\image-20210621114444001.png" alt="image-20210621114444001" style="zoom:67%;" />

因此，上述过程可以总结为：

将进程的页表始址和页表长度加载到页表寄存器中 => 分页地址机构将程序的逻辑地址中的页号去页表检索 => 找到对应的物理块后，装入物理地址寄存器中 => 将逻辑地址中的页内地址装入物理地址寄存器中

例如：下图的页表存放着 16 个页，这 16 个页需要用 4 个比特位来进行索引定位。对于逻辑地址（0010 000000000100），前 4 位是存储页面号（0010十进制为2，对应下面的页表的第2项），后 12 位存储页内地址（也叫偏移量）。在基地址变换结构中，读取到了页表第2项的内容为110 1，该内容的最后一位表示是否存在于内存中（1 表示存在），读取到后，将改地址送入物理地址中，并把后12位的页内地址一起放入物理地址中，最后的地址为 110 000000000100

<img src="Image\19" alt="68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f63663433383661312d353863392d346563612d613137662d6531326231653937373065622e706e67" style="zoom:67%;" />

#### 具有快表的地址变换机构

> 由于页表是存放在内存中的，这使CPU在每存取一个数据时，都要两次访问内存。第一次是访问内存中的页表，从中找到指定页的物理块号，再将块号与页内偏移量拼接，以形成物理地址。第二次访问内存时，才是从第一次所得地址中获得所需数据(或向此地址中写入数据)。因此，采用这种方式将使计算机的处理速度降低近为了提高地址变换速度，可在地址变换机构中增设一个具有**并行查寻能力的特殊高速缓冲寄存器**，又称为“联想寄存器" (Associative Memory)，或称为“**快表**"

如图4-16所示，具有快表的地址变换机构过程如下：

（1）在CPU给出有效地址后，由地址变换机构自动地将页号P送入高速缓冲寄存器

（2）将此页号与高速缓存中的所有页号进行比较，若其中有与此相匹配的页号，便表示所要访问的页表项在快表中

（3）若在快表中找到了，则**直接从快表中读出该页所对应的物理块号，并送到物理地址寄存器**中

（4）若在快表中未找到对应的页表项，则还须**再访问内存中的页表**，找到后，把从页表项中读出的物理块号送往地址寄存器；同时再将此页表项存入快表的一个寄存器单元中

（5）但如果快表已满，则OS必须找到一个老的且已被认为是不再需要的页表项，将它换出，换出的算法成为**页面置换算法**

<img src="Image\image-20210621114631071.png" alt="image-20210621114631071" style="zoom:50%;" />

#### 访问内存的有效时间

从进程发出指定逻辑地址的访问请求，经过地址变换，到在内存中找到对应的实际物理地址单元并取出数据，所需要花费的总时间，称为**内存的有效访问时间**(Effective AccessTime，EAT)。

假设访问一次内存的时间为t，在基本分页存储管理方式中，有效访问时间分为第一次访问内存时间(即查找页表对应的页表项所耗费的时间t)与第二次访问内存时间(即将页表项中的物理块号与页内地址拼接成实际物理地址所耗费的时间t)之和:
$$
EAT=t+t=2t
$$
在引入快表的分页存储管理方式中，有效访问时间的计算公式即为:
$$
EAT=a\times \lambda +(t+\lambda)(1-a)+t=2t+\lambda-t\times a
\\ \lambda ——查找快表所需时间
\\ a——命中率
\\ t——访问一次内存所需时间
$$

#### 两级页表

针对难于找到大的连续的内存空间来存放页表的问题，可利用将页表进行分页的方法，使每个页面的大小与内存物理块的大小相同,并为它们进行编号,即依次为0#页、1#页，…,n#页，然后离散地将各个页面分别存放在不同的物理块中。具有两级页表的逻辑地址如下：

<img src="Image\image-20210711013135147.png" alt="image-20210711013135147" style="zoom:80%;" />

在地址变换机构中，同样需要增设一个外层页表寄存器，用于存放外层页表的始址，并利用逻辑地址中的外层页号作为外层页表的索引，从中找到指定页表分页的始址，再利用P2作为指定页表分页的索引，找到指定的页表项，其中即含有该页在内存的物理块号，用该块号Р和页内地址d即可构成访问的内存物理地址。图4-18示出了两级页表时的地址变换机构

<img src="Image\image-20210711013206855.png" alt="image-20210711013206855" style="zoom:80%;" />

### 分段存储管理

分段存储管理的优点：方便编程、信息共享、信息保护、动态增长、动态链接

在分段存储管理方式中，作业的地址空间被划分为若干个段，每个段定义了一组逻辑信息。例如，有主程序段MAIN、子程序段X、数据段D及栈段S等。如图4-19所示。每个段都有自己的名字。

为了实现简单起见，通常可用一个段号来代替段名，每个段都从0开始编址，并采用一段连续的地址空间。段的长度由相应的逻辑信息组的长度决定，因此各段的长度并不相等。**整个作业的地址空间由于被分成多个段，所以呈现出二维特性，亦即，每个段既包含了一部分地址空间，又标识了逻辑关系**。其逻辑地址由**段号和段内地址**所组成。

<img src="Image\image-20210621151325885.png" alt="image-20210621151325885" style="zoom:50%;" />

#### 段表

在前面所介绍的动态分区分配方式中，系统为整个进程分配一个连续的内存空间。而在分段式存储管理系统中，则是为**每个分段分配一个连续的分区**。进程中的各个段，可以离散地装入内存中不同的分区中。为保证程序能正常运行，就必须能从物理内存中找出每个逻辑段所对应的位置。

如上图 4-19 所示，在系统中为每个进程建立一张**段映射表**，简称“段表” 。每个段在表中占有一个表项，其中记录了该段在内存中的起始地址(又称为“基址”)和段的长度，段表可以存放在一组寄存器中，以利于提高地址转换速度。但更常见的方法是将段表放在内存中。在配置了段表后，执行中的进程可通过查找段表，找到每个段所对应的内存区。可见**段表是用于实现从逻辑段到物理内存区的映射的**。

<img src="Image\image-20210621151623288.png" alt="image-20210621151623288" style="zoom:50%;" />

#### 地址变换机构

如图4-20所示的分段系统的地址变换过程，为了实现进程从逻辑地址到物理地址的变换功能，在系统中设置了**段表寄存器**，用于存放**段表始址和段表长度TL**，在进行地址变换时，系统将逻辑地址中的段号与段表长度TL进行比较。若s> TL，表示段号太大，是访问越界，于是产生**越界中断信号**。若未越界，则根据段表的始址和该段的段号，计算出该段对应段表项的位置，从中读出该段在内存的起始地址。然后，再检查段内地址d是否超过该段的段长SL。若超过，即dSL，同样发出越界中断信号。若未越界，则**将该段的基址d与段内地址相加，即可得到要访问的内存物理地址**。

<img src="Image\image-20210621152051735.png" alt="image-20210621152051735" style="zoom:50%;" />

上面的图可以这么理解：右上角是逻辑地址，包含了断号和位移量，然后通过段表寄存器（控制寄存器），这个寄存器里面存储了段表，可以查到对应段号的物理基址，然后物理基址和逻辑地址的位移量相加，就得到了物理地址

**分页与分段的主要区别**

- 对程序员的透明性：分页透明，但是分段需要程序员显式划分每个段。
- 地址空间的维度：分页是一维地址空间，分段是二维的。
- 大小是否可以改变：页的大小不可变，段的大小可以动态改变。
- 出现的原因：分页主要用于实现虚拟内存，从而获得更大的地址空间；分段主要是为了使程序和数据可以被划分为逻辑上独立的地址空间并且有助于共享和保护。



### 段页式存储管理

分页系统以页面作为内存分配的基本单位，能有效地提高内存利用率，而分段系统以段作为内存分配的基本单位，它能够更好地满足用户多方面的需要。而段页式存储管理方式**既有分段系统的便于实现、分段可共享、易于保护、可动态链接**等一系列优点，又能像分页系统那样，很好地**解决内存的外部碎片**问题

**基本原理**

段页式系统的基本原理是分段和分页原理的结合，即先将用户程序分成若干个段，再把每个段分成若干个页，并为每一个段赋予一个段名。图4-23(a)示出了一个作业地址空间的结构。该作业有三个段：主程序段、子程序段和数据段（下图显示了这些分段大小都不一样，也就是可以动态改变），页面大小为4KB。在段页式系统中，其逻辑地址由**段号、段内页号及页内地址**三部分所组成，如图4-23(b)所示。

<img src="Image\33" alt="image-20210621152706367" style="zoom:50%;" />

在段页式系统中，为了实现从逻辑地址到物理地址的变换，系统中需要同时配置段表和页表。段表的内容与分段系统略有不同，它不再是内存始址和段长，而是页表始址和页表长度。图4-24示出了利用段表和页表进行从用户地址空间到物理(内存)空间的映射。

<img src="Image\35" alt="image-20210621152745396" style="zoom:50%;" />

**地址变换过程**

在段页式系统中，为了便于实现地址变换，须配置一个**段表寄存器**，其中存放段表始址和段长TL。如图4-25为段页式系统中的地址变换机构，其变换过程如下：

（1）利用段号s，将它与段长TL进行比较。若S<TL，表示未越界

（2）利用段表始址和段号来求出该段所对应的段表项在段表中的位置，从中得到该段的**页表始址**，

（3）利用逻辑地址中的段内页号P来获得对应页的页表项位置，从中读出该页所在的物理块号b，再利用块号b和页内地址来构成物理地址。

可见，在段页式系统中，为了获得一条指令或数据，须三次访问内存

- 第一次访问是访问内存中的段表，从中取得页表始址
- 第二次访问是访问内存中的页表，从中取出该页所在的物理块号，并将该块号与页内地址一起形成指令或数据的物理地址
- 第三次访问才是真正从第二次访问所得的地址中取出指令或数据。

<img src="Image\36" alt="image-20210621152815837" style="zoom:50%;" />

# 虚拟存储器

虚拟存储器也叫虚拟内存，可以实现了内存扩充功能。该功能并非是从物理上实际地扩大内存的容量，而是从逻辑上实现对内存容量的扩充，让用户所感觉到的内存容量比实际内存容量大得多。于是便可以**让比内存空间更大的程序运行**，或者**让更多的用户程序并发运行**。这样既满足了用户的需要，又改善了系统的性能

从上面的描述中可以看出，**虚拟内存允许程序不用将地址空间中的每一页都映射到物理内存**，也就是说一个程序不需要全部调入内存就可以运行，这使得有限的内存运行大程序成为可能。如下图所示，有一台计算机可以产生 16 位地址，那么一个程序的地址空间范围是 0~64K（2^16/1024）。该计算机只有 32KB 的物理内存，虚拟内存技术允许该计算机运行一个 64K 大小的程序。

![68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f37623238316231652d303539352d343032622d616533352d3863393130383463333363312e706e67](Image\18)

在 Windows 中，可按照如下设置虚拟内存

<img src="Image\image-20210621160820225.png" alt="image-20210621160820225" style="zoom:50%;" />

## 常规存储管理的局限性

上一章描述的内存管理方式为传统存储器管理方式，主要有如下以下特点

- **一次性**，是指作业必须一次性地全部装入内存后方能开始运行。
- **驻留性**，是指作业被装入内存后，整个作业都一直驻留在内存中，其中任何部分都不会被换出，直至作业运行结束

由此可以看出，一次性及驻留性特征使得许多在程序运行中不用或暂时不用的程序(数据)**占据了大量的内存空间**，而一些需要运行的作业又无法装入运行，显然，这是在**浪费宝贵的内存资源**。现在要研究的问题是:一次性及驻留性特征是否是程序在运行时所必需的和不可改变的。



## 一些概念

**虚拟内存**

指具有请求调入功能和置换功能，能从逻辑上对内存容量加以扩充的一种存储器系统。其逻辑容量由内存容量和外存容量之和所决定，其运行速度接近于内存速度，而每位的成本却又接近于外存

**虚拟内存的特点**

- **多次性**。指一个作业中的程序和数据无需在作业运行时一次性地全部装入内存，而是允许被分成多次调入内存运行，即只需将当前要运行的那部分程序和数据装入内存即可开始运行。以后每当要运行到尚未调入的那部分程序时，再将它调入。
- **对换性**。指一个作业中的程序和数据，无须在作业运行时一直常驻内存，而是在进程运行期间，允许将那些暂不使用的代码和数据从内存调至外存的对换区(换出)，待以后需要时再将它们从外存调至内存(换进)，甚至还允许将暂时不运行的进程调至外存，待它们重又具备运行条件时再调入内存。
- **虚拟性**。指能够从逻辑上扩充内存容量，使用户所看到的内存容量远大于实际内存容量。

虚拟内存的实现，一般是建立在离散的分配存储管理方式的基础上。**目前，所有的虚拟存储器都是采用请求分页系统和请求分段系统实现的**

## 虚拟内存的工作原理

应用程序在运行之前没有必要将之全部装入内存，而仅须将那些当前要运行的少数页面或段先装入内存便可运行，其余部分暂留在盘上。

程序在运行时，如果它所要访问的页(段)已调入内存，便可继续执行下去;但如果程序所要访问的页(段)尚未调入内存(称为缺页或缺段)，便发出**缺页(段)中断请求**，此时Os将利用请求调页(段)功能将它们调入内存，以使进程能继续执行下去。

如果此时内存已满，无法再装入新的页(段)， os还须再利用**页(段)的置换功能**，将内存中暂时不用的页(段)调至盘上，腾出足够的内存空间后，再将要访问的页(段)调入内存，使程序继续执行下去。这样，便可使一个大的用户程序在较小的内存空间中运行，也可在内存中同时装入更多的进程，使它们并发执行。

为了实现请求分页，系统必须提供一定的硬件支持。计算机系统除了要求一定容量的内存和外存外，还需要有**请求页表机制、缺页中断机构以及地址变换机构**。

### 数据结构

在请求分页系统中需要的主要数据结构是请求页表，其基本作用仍然是将用户地址空间中的逻辑地址映射为内存空间中的物理地址。为了满足页面换进换出的需要，在请求页表中又增加了四个字段。这样，在请求分页系统中的每个页表应含以下诸项

<img src="Image\image-20210621162158023.png" alt="image-20210621162158023" style="zoom:50%;" />

- 状态位(存在位)P：用于指示该页是否已调入内存
- 访问字段A：用于记录本页在一段时间内被访问的次数，或记录本页最近已有多长时间未被访问
- 修改位M：标识该页在调入内存后是否被修改过。由于内存中的每一页都在外存上保留一份副本，因此，在置换该页时，若未被修改，就不需再将该页写回到外存上，以减少系统的开销和启动磁盘的次数;若已被修改，则必须将该页重写到外存上，以保证外存中所保留的副本始终是最新的
- 外存地址：用于指出该页在外存上的地址，通常是物理块号

### 缺页中断机制

在请求分页系统中，每当所要访问的页面不在内存时，便产生一缺页中断，请求os将所缺之页调入内存。

### 地址变换机构

请求分页系统中的地址变换机构是在分页系统地址变换机构的基础上，为实现虚拟存·储器，再增加了某些功能所形成的，如产生和处理缺页中断，以及从内存中换出一页的功能等等。图5-2示出了请求分页系统中的地址变换过程。

<img src="Image\image-20210621162624776.png" alt="image-20210621162624776" style="zoom:50%;" />

### 页面置换算法【重要】

应将哪个页面调出，须根据一定的算法来确定。通常，把选择换出页面的算法称为**页面置换算法**(Page-Replacement Algorithms)。置换算法的好坏将直接影响到系统的性能。

页面置换算法和缓存淘汰策略类似，可以将内存看成磁盘的缓存。在缓存系统中，缓存的大小有限，**当有新的缓存到达时，需要淘汰一部分已经存在的缓存，这样才有空间存放新的缓存数据**。

页面置换算法的主要目标是使页面置换频率最低（也可以说缺页率最低）

缺页率 `f` 公式如下：
$$
f=\frac{F}{A}\\ 其中，A=S+F   \\ S-访问页面成功的次数 \\ F- 方面页面失败的次数 \\ A-该进程总的页面访问次数
$$


**1. 最佳置换算法（理想状态）**

> OPT， Optimal replacement algorithm

所选择的被换出的页面将是最长时间内不再被访问，通常可以保证获得最低的缺页率。这是一种理论上的算法，因为无法知道一个页面多长时间不再被访问。

例如：一个系统为某进程分配了三个物理块，并按照如下顺序进行页面引用

```
7，0，1，2，0，3，0，4，2，3，0，3，2，1，2，0，1，7，0，1
```

开始运行时，先将 7， 0， 1 三个页面装入内存。当进程要访问页面 2 时，产生缺页中断，会将页面 7 换出，因为页面 7 再次被访问的时间最长，在倒数第三个（但是实际上不知道 7 什么时候才回访问）。

**2. 先进先出**

> FIFO， First In First Out

将最先进入的页面替换出去。该算法实现简单，只需把一个进程已调入内存的页面，按先后次序链接成一个队列，并设置一个指针，称为替换指针，使它总是指向最老的页面。该算法会将那些经常被访问的页面换出，导致缺页率升高。

**3. 最近最久未使用**

> LRU， Least Recently Used

通过过去使用页面的情况来将页面换出，即将最近最久未使用的页面换出。

该算法如下：在内存中**维护一个所有页面的链表**。当一个页面被访问时，将这个页面移到链表表头。这样就能保证链表表尾的页面是最近最久未访问的，以下序列一次访问时如下图所示

```
4，7，0，7，1，0，1，2，1，2，6
```

![68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f65623835393232382d633066322d346263652d393130642d6439663736393239333532622e706e67](Image\20)

**4. 最少使用**

> Least Frequently Used， LFU

算法与 LRU 类似，不过记录的是最少使用的

**4. 最近未使用**

> NRU， Not Recently Used

每个页面都有两个状态位：R 与 M，当页面被访问时设置页面的 R=1，当页面被修改时设置 M=1。其中 R 位会定时被清零。可以将页面分成以下四类：

- R=0，M=0
- R=0，M=1
- R=1，M=0
- R=1，M=1

当发生缺页中断时，NRU 算法随机地从**类编号**最小的非空类中挑选一个页面将它换出。NRU 优先换出已经被修改的脏页面（R=0，M=1），而不是被频繁使用的干净页面（R=1，M=0）。

**5. 第二次机会算法**

FIFO 算法可能会把经常使用的页面置换出去，为了避免这一问题，对该算法做一个简单的修改：

当页面被访问 (读或写) 时设置该页面的 R 位为 1。需要替换的时候，检查最老页面的 R 位。如果 R 位是 0，那么这个页面既老又没有被使用，可以立刻置换掉；如果是 1，就将 R 位清 0，并把该页面放到链表的尾端，修改它的装入时间使它就像刚装入的一样，然后继续从链表的头部开始搜索。

![68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f65636638616435642d353430332d343862392d623665372d6632653230666665386663612e706e67](Image\21)



### 请求分段系统

实现原理以及所需要的硬件支持上与请求分页系统类似



# 输入输出系统

IO系统管理的主要对象是IO设备和相应的设备控制器。其最主要的任务是，完成用户提出的I/O 请求，提高I/O速率，以及提高设备的利用率，并能为更高层的进程方便地使用这些设备提供手段。

<img src="Image\image-20210711013923257.png" alt="image-20210711013923257" style="zoom: 80%;" />



## 磁盘存储器

俗称硬盘，可包括一个或多个物理盘片

<img src="Image\image-20210621165228330.png" alt="image-20210621165228330" style="zoom:50%;" />

对于单个物理片来说，其主要组成如下

- 盘面（Platter）：一个磁盘有多个盘面；
- 磁道（Track）：盘面上的圆形带状区域，一个盘面可以有多个磁道；
- 扇区（Track Sector）：磁道上的一个弧段，一个磁道可以有多个扇区，它是最小的物理储存单位，目前主要有 512 bytes 与 4 K 两种大小；
- 磁头（Head）：与盘面非常接近，**能够将盘面上的磁场转换为电信号（读）**，**或者将电信号转换为盘面的磁场（写）**；
- 制动手臂（Actuator arm）：用于在磁道之间移动磁头；
- 主轴（Spindle）：使整个盘面转动。

<img src="Image\25"  style="zoom:80%;" />

## 磁盘调度算法

读写一个磁盘块的时间的影响因素有：

- 旋转时间（主轴转动盘面，使得磁头移动到适当的扇区上）
- 寻道时间（制动手臂移动，使得磁头移动到适当的磁道上）
- 实际的数据传输时间

其中，寻道时间最长，因此**磁盘调度的主要目标是使磁盘的平均寻道时间最短**。

**1. 先来先服务**

> FCFS， First Come First Served

按照磁盘请求的顺序进行调度。

优点是公平和简单。缺点也很明显，因为未对寻道做任何优化，使平均寻道时间可能较长。

**2. 最短寻道时间优先**

> SSTF， Shortest Seek Time First

优先调度与当前磁头所在磁道距离最近的磁道。

虽然平均寻道时间比较低，但是不够公平。如果新到达的磁道请求总是比一个在等待的磁道请求近，那么在等待的磁道请求会一直等待下去，也就是出现饥饿现象。具体来说，两端的磁道请求更容易出现饥饿现象。

<img src="Image\26" alt="68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f34653234383565342d333462642d343936372d396630322d3063303933623739376161612e706e67" style="zoom: 80%;" />

**3. 电梯算法**

> SCAN

电梯总是保持一个方向运行，直到该方向没有请求为止，然后改变运行方向。

电梯算法（扫描算法）和电梯的运行过程类似，总是按一个方向来进行磁盘调度，直到该方向上没有未完成的磁盘请求，然后改变方向。

因为考虑了移动方向，因此所有的磁盘请求都会被满足，解决了 SSTF 的饥饿问题。



<img src="Image\27" alt="68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f32373163653038662d633132342d343735662d623439302d6265343466656463366432652e706e67" style="zoom:80%;" />



# 文件管理

## 文件和文件系统





## 文件的逻辑结构









## 文件目录







## 文件共享





## 文件保护







# 磁盘存储器的管理



## 外存





## 文件存储空间的管理





## 提高磁盘 IO 速度的途径







## 提高磁盘可靠性的技术



## 数据一致性控制





# 操作系统接口



## 用户接口





## Shell 命令语言







## 联机命令接口的实现





## 系统调用的概念和类型





## UNIX 系统调用



## 系统调用的实现









# 多处理机操作系统



# 多媒体操作系统



# 保护和安全





